{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e377d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import everything\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import mysql.connector\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from bisect import bisect_left, bisect_right\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# the path to the .env file\n",
    "dotenv_path = \"/Users/tristanbrigham/Desktop/Classes/CPSC 6440/FinalProject/Submission/.env\"\n",
    "load_dotenv(dotenv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8461386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data\n",
    "with open(\"training_gnn_datasets_snapshot.pkl\", \"rb\") as f:\n",
    "    snapshot = pickle.load(f)\n",
    "    df_trades = snapshot[\"df_trades\"]\n",
    "    df_mo = snapshot[\"df_mo\"]\n",
    "    clob_outcome_dict = snapshot[\"clob_outcome_dict\"]\n",
    "    df_markets = snapshot[\"df_markets\"]\n",
    "    df_market_events = snapshot[\"df_market_events\"]\n",
    "    df_event_series = snapshot[\"df_event_series\"]\n",
    "\n",
    "print(\"loaded trades shape:\", df_trades.shape)\n",
    "print(df_trades.head())\n",
    "print(\"number of clob tokens in dict:\", len(clob_outcome_dict))\n",
    "print(\"mapping tables shapes:\",\n",
    "      \"markets:\", df_markets.shape,\n",
    "      \"market_events:\", df_market_events.shape,\n",
    "      \"event_series:\", df_event_series.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaf2e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the times that things ran\n",
    "\n",
    "# trade_time is already datetime\n",
    "df_trades = df_trades.sort_values(\"trade_time\").reset_index(drop=True)\n",
    "\n",
    "# make sure that the timestamp is numeric\n",
    "df_trades[\"timestamp\"] = pd.to_numeric(df_trades[\"timestamp\"], errors=\"coerce\")\n",
    "df_trades = df_trades.dropna(subset=[\"timestamp\"])\n",
    "\n",
    "\n",
    "# get the number of trades and split\n",
    "n = len(df_trades)\n",
    "idx_train_end = int(0.8 * n)\n",
    "idx_val_end = int(0.9 * n)\n",
    "\n",
    "\n",
    "# get the train, val, and test sets\n",
    "df_train = df_trades.iloc[:idx_train_end].copy()\n",
    "df_val = df_trades.iloc[idx_train_end:idx_val_end].copy()\n",
    "df_test = df_trades.iloc[idx_val_end:].copy()\n",
    "\n",
    "print(\"train rows:\", len(df_train))\n",
    "print(\"val rows:\", len(df_val))\n",
    "print(\"test rows:\", len(df_test))\n",
    "\n",
    "# training period length in years\n",
    "t_min_train = df_train[\"trade_time\"].min()\n",
    "t_max_train = df_train[\"trade_time\"].max()\n",
    "train_period_seconds = float((t_max_train - t_min_train).total_seconds())\n",
    "train_period_years = train_period_seconds / (365.25 * 24 * 60 * 60)\n",
    "\n",
    "print(\"train period years:\", train_period_years)\n",
    "\n",
    "# minimum number of trades in the training set required for a wallet to be included\n",
    "# so that we don't get a bunch of rando wallets\n",
    "MIN_TRAIN_TRADES = 15 \n",
    "\n",
    "train_trade_counts = df_train.groupby(\"proxy_wallet\")[\"id\"].count()\n",
    "eligible_wallets = set(train_trade_counts[train_trade_counts >= MIN_TRAIN_TRADES].index)\n",
    "\n",
    "print(\"wallets with at least\", MIN_TRAIN_TRADES, \"trades in training:\", len(eligible_wallets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0845da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enrich the data that we have\n",
    "\n",
    "# merge condition_id into the market_id\n",
    "df_markets_trim = df_markets[[\"id\", \"conditionId\"]].dropna(subset=[\"conditionId\"]).copy()\n",
    "df_markets_trim.rename(columns={\"id\": \"market_id\"}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# merge the dataframes\n",
    "df_trades = df_trades.merge(\n",
    "    df_markets_trim,\n",
    "    how=\"left\",\n",
    "    left_on=\"condition_id\",\n",
    "    right_on=\"conditionId\"\n",
    ")\n",
    "\n",
    "# make sure that we don't have any duplicate columns\n",
    "df_trades.drop(columns=[\"conditionId\"], inplace=True)\n",
    "\n",
    "# merge market_id into the event_id\n",
    "if not df_market_events.empty:\n",
    "    df_me_trim = df_market_events[[\"market_id\", \"event_id\"]].dropna(subset=[\"market_id\"])\n",
    "    df_trades = df_trades.merge(\n",
    "        df_me_trim,\n",
    "        how=\"left\",\n",
    "        on=\"market_id\"\n",
    "    )\n",
    "else:\n",
    "    df_trades[\"event_id\"] = np.nan\n",
    "\n",
    "# merge event_id into the series_id\n",
    "if not df_event_series.empty:\n",
    "    df_es_trim = df_event_series[[\"event_id\", \"series_id\"]].dropna(subset=[\"event_id\"])\n",
    "    df_trades = df_trades.merge(\n",
    "        df_es_trim,\n",
    "        how=\"left\",\n",
    "        on=\"event_id\"\n",
    "    )\n",
    "else:\n",
    "    df_trades[\"series_id\"] = np.nan\n",
    "\n",
    "\n",
    "# print some base statistics\n",
    "print(\"df_trades with market/event/series columns:\")\n",
    "print(df_trades[[\"proxy_wallet\", \"condition_id\", \"market_id\", \"event_id\", \"series_id\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe030e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the prices of the assets in question over time\n",
    "\n",
    "# extract needed columns\n",
    "df_prices = df_trades[[\"trade_time\", \"asset\", \"price\"]].copy()\n",
    "df_prices[\"price\"] = df_prices[\"price\"].astype(float)\n",
    "\n",
    "# make sure timestamps are sorted\n",
    "df_prices = df_prices.sort_values(\"trade_time\").reset_index(drop=True)\n",
    "\n",
    "# all unique timestamps across all trades \n",
    "unique_times = pd.Index(df_prices[\"trade_time\"].sort_values().unique())\n",
    "\n",
    "# for each token, get the price at each timestamp\n",
    "token_price_paths = {}\n",
    "\n",
    "for token_id, g in tqdm(\n",
    "    df_prices.groupby(\"asset\", sort=False),\n",
    "    total=df_prices[\"asset\"].nunique(),\n",
    "    desc=\"building sparse token price paths\"\n",
    "):\n",
    "    g = g.sort_values(\"trade_time\")\n",
    "\n",
    "    # collapse duplicate timestamps per token by taking the last trade at each timestamp\n",
    "    g_token = (\n",
    "        g[[\"trade_time\", \"price\"]]\n",
    "        .groupby(\"trade_time\", as_index=True)[\"price\"]\n",
    "        .last()\n",
    "        .to_frame()\n",
    "    )\n",
    "\n",
    "    token_price_paths[token_id] = g_token\n",
    "\n",
    "print(\"number of tokens with sparse price series:\", len(token_price_paths))\n",
    "\n",
    "# see the prices of one example token\n",
    "if token_price_paths:\n",
    "    example_token = next(iter(token_price_paths.keys()))\n",
    "    print(\"example token:\", example_token)\n",
    "    print(token_price_paths[example_token].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e18de0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the asset that we found with the highest overall variance\n",
    "token_price_change_counts = {}\n",
    "\n",
    "# count the number of price changes for each token\n",
    "for token_id, df_token in token_price_paths.items():\n",
    "    \n",
    "    # getting the price values\n",
    "    prices = df_token[\"price\"].values\n",
    "    if len(prices) < 2:\n",
    "        continue\n",
    "    # count steps where price changes by a good amount\n",
    "    num_changes = np.sum(np.abs(np.diff(prices)) > 1e-6)  \n",
    "    token_price_change_counts[token_id] = num_changes\n",
    "\n",
    "if not token_price_change_counts:\n",
    "    raise ValueError(\"no tokens with nontrivial price series found.\")\n",
    "\n",
    "# get the one that has changed price the most times overall\n",
    "most_active_token = max(token_price_change_counts, key=token_price_change_counts.get)\n",
    "print(\"token with most price changes along the way:\", most_active_token)\n",
    "print(\"number of price changes:\", token_price_change_counts[most_active_token])\n",
    "\n",
    "df_most_active = token_price_paths[most_active_token]\n",
    "\n",
    "var_most_active = np.var(df_most_active[\"price\"].values)\n",
    "print(\"variance of this token's price path:\", var_most_active)\n",
    "\n",
    "\n",
    "# get the price plotted over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_most_active.index, df_most_active[\"price\"], marker='.', lw=1)\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"price\")\n",
    "plt.title(f\"price trajectory with most changes for token {most_active_token}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddab83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict to eligible wallets only\n",
    "df_trades_wallet = df_trades[df_trades[\"proxy_wallet\"].isin(eligible_wallets)][\n",
    "    [\"trade_time\", \"proxy_wallet\", \"asset\", \"side\", \"size\", \"price\"]\n",
    "].copy()\n",
    "\n",
    "\n",
    "# get the size and price as floats\n",
    "df_trades_wallet[\"size\"] = df_trades_wallet[\"size\"].astype(float)\n",
    "df_trades_wallet[\"price\"] = df_trades_wallet[\"price\"].astype(float)\n",
    "\n",
    "# sort the trades by time\n",
    "df_trades_wallet = df_trades_wallet.sort_values(\"trade_time\").reset_index(drop=True)\n",
    "\n",
    "# compute share changes\n",
    "df_trades_wallet[\"share_change\"] = np.where(\n",
    "    df_trades_wallet[\"side\"] == \"BUY\",\n",
    "    df_trades_wallet[\"size\"],\n",
    "    -df_trades_wallet[\"size\"]\n",
    ")\n",
    "\n",
    "# compute cash changes\n",
    "df_trades_wallet[\"cash_change\"] = np.where(\n",
    "    df_trades_wallet[\"side\"] == \"BUY\",\n",
    "    -df_trades_wallet[\"size\"] * df_trades_wallet[\"price\"],\n",
    "    df_trades_wallet[\"size\"] * df_trades_wallet[\"price\"]\n",
    ")\n",
    "\n",
    "# group by the wallet and the asset combination\n",
    "wallet_initial_shares = defaultdict(dict)\n",
    "groupby_wallet_asset = df_trades_wallet.groupby([\"proxy_wallet\", \"asset\"])\n",
    "\n",
    "# iterate through the pairs and investigate what we find\n",
    "for (wallet, asset), grp in tqdm(groupby_wallet_asset, desc=\"wallet-asset pairs\"):\n",
    "    share_change = grp[\"share_change\"].values\n",
    "    cum_shares = np.cumsum(share_change)\n",
    "    min_cum = cum_shares.min()\n",
    "    init_shares = max(0.0, -float(min_cum))\n",
    "    wallet_initial_shares[wallet][asset] = init_shares\n",
    "\n",
    "# compute running statistics\n",
    "wallet_initial_cash = {}\n",
    "groupby_wallet = df_trades_wallet.groupby(\"proxy_wallet\")\n",
    "for wallet, grp in tqdm(groupby_wallet, desc=\"wallets\"):\n",
    "    cash_change = grp[\"cash_change\"].values\n",
    "    cum_cash = np.cumsum(cash_change)\n",
    "    min_cash = cum_cash.min()\n",
    "    init_cash = max(0.0, -float(min_cash))\n",
    "    wallet_initial_cash[wallet] = init_cash\n",
    "\n",
    "print(\"wallets with inferred initial holdings:\", len(wallet_initial_cash))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca7acd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a downsampled global time grid\n",
    "all_times = unique_times\n",
    "\n",
    "# downsamplignt o make things faster to run\n",
    "num_grid = min(400, len(all_times)) \n",
    "\n",
    "grid_indices = np.linspace(0, len(all_times) - 1, num_grid).astype(int)\n",
    "grid_times = all_times[grid_indices]\n",
    "\n",
    "print(\"number of grid times:\", len(grid_times))\n",
    "print(\"grid start:\", grid_times[0], \"grid end:\", grid_times[-1])\n",
    "\n",
    "# map tokens to indices and prebuild price matrix aligned with grid_times\n",
    "token_list = sorted(token_price_paths.keys())\n",
    "token_to_idx = {tok: i for i, tok in enumerate(token_list)}\n",
    "num_tokens = len(token_list)\n",
    "\n",
    "def prices_on_grid(df_token, grid_times):\n",
    "    # df_token is indexed by its own trade_time, with the price column\n",
    "    tok_times = df_token.index.to_numpy()\n",
    "    tok_prices = df_token[\"price\"].to_numpy()\n",
    "\n",
    "    if tok_times.size == 0:\n",
    "        # no trades for this token\n",
    "        return np.zeros(len(grid_times), dtype=np.float32)\n",
    "\n",
    "    # for each grid time, take last trade price at or before that time\n",
    "    idx = np.searchsorted(tok_times, grid_times, side=\"right\") - 1\n",
    "\n",
    "    # if a grid time is before the first trade, use the first trade price\n",
    "    idx[idx < 0] = 0\n",
    "    idx = np.clip(idx, 0, tok_times.size - 1)\n",
    "\n",
    "    return tok_prices[idx].astype(np.float32)\n",
    "\n",
    "# the price grid is a matrix of the prices of the tokens at the grid times\n",
    "price_grid = np.zeros((num_tokens, num_grid), dtype=np.float32)\n",
    "for tok, i_tok in tqdm(token_to_idx.items(), desc=\"building price grid\"):\n",
    "    df_tok = token_price_paths[tok]\n",
    "    price_grid[i_tok, :] = prices_on_grid(df_tok, grid_times)\n",
    "\n",
    "# wallet list and index mapping\n",
    "wallet_list = sorted(df_trades_wallet[\"proxy_wallet\"].unique())\n",
    "wallet_to_idx = {w: i for i, w in enumerate(wallet_list)}\n",
    "idx_to_wallet = {i: w for w, i in wallet_to_idx.items()}\n",
    "num_wallets = len(wallet_list)\n",
    "\n",
    "print(\"number of wallets with trades (eligible):\", num_wallets)\n",
    "\n",
    "# portfolio value matrix\n",
    "V = np.zeros((num_grid, num_wallets), dtype=np.float64)\n",
    "\n",
    "# prepare numpy arrays for trades to avoid pandas overhead in inner loop\n",
    "dfw = df_trades_wallet.sort_values(\"trade_time\").reset_index(drop=True)\n",
    "time_arr = dfw[\"trade_time\"].to_numpy()\n",
    "wallet_arr = dfw[\"proxy_wallet\"].to_numpy()\n",
    "asset_arr = dfw[\"asset\"].to_numpy()\n",
    "side_arr = dfw[\"side\"].to_numpy()\n",
    "size_arr = dfw[\"size\"].to_numpy(dtype=float)\n",
    "price_arr = dfw[\"price\"].to_numpy(dtype=float)\n",
    "n_trades = len(dfw)\n",
    "\n",
    "# initialize per-wallet state\n",
    "# that we can continue from\n",
    "wallet_cash = {}\n",
    "wallet_pos = {}\n",
    "\n",
    "for w in tqdm(wallet_list, desc=\"init wallet state\"):\n",
    "    wallet_cash[w] = float(wallet_initial_cash.get(w, 0.0))\n",
    "    wallet_pos[w] = dict(wallet_initial_shares.get(w, {}))\n",
    "\n",
    "# get the portfolio value for the wallet at a given point in time\n",
    "def compute_wallet_value(wallet, k):\n",
    "    \n",
    "    # get the cash and positions of the wallet\n",
    "    val = wallet_cash[wallet]\n",
    "    pos = wallet_pos[wallet]\n",
    "\n",
    "    # if the wallet has no positions, return the cash\n",
    "    if not pos:\n",
    "        return val\n",
    "    for asset, shares in pos.items():\n",
    "        if shares == 0.0:\n",
    "            continue\n",
    "        tok_idx = token_to_idx.get(asset)\n",
    "        if tok_idx is None:\n",
    "            continue\n",
    "        p = price_grid[tok_idx, k]\n",
    "        val += shares * p\n",
    "    return val\n",
    "\n",
    "# loop over the grid times\n",
    "trade_idx = 0\n",
    "active_wallets = set()\n",
    "\n",
    "for k, t in enumerate(tqdm(grid_times, desc=\"grid time steps\")):\n",
    "    # carry forward previous values by default\n",
    "    if k > 0:\n",
    "        V[k, :] = V[k - 1, :]\n",
    "\n",
    "    # process trades up to and including time t\n",
    "    while trade_idx < n_trades and time_arr[trade_idx] <= t:\n",
    "        w = wallet_arr[trade_idx]\n",
    "        a = asset_arr[trade_idx]\n",
    "        s = side_arr[trade_idx]\n",
    "        q = size_arr[trade_idx]\n",
    "        p = price_arr[trade_idx]\n",
    "\n",
    "        if w not in wallet_cash:\n",
    "            wallet_cash[w] = 0.0\n",
    "            wallet_pos[w] = {}\n",
    "\n",
    "        if a not in wallet_pos[w]:\n",
    "            wallet_pos[w][a] = 0.0\n",
    "\n",
    "        if s == \"BUY\":\n",
    "            wallet_pos[w][a] += q\n",
    "            wallet_cash[w] -= q * p\n",
    "        else:\n",
    "            wallet_pos[w][a] -= q\n",
    "            wallet_cash[w] += q * p\n",
    "\n",
    "        if wallet_pos[w][a] < 0 and wallet_pos[w][a] > -1e-9:\n",
    "            wallet_pos[w][a] = 0.0\n",
    "\n",
    "        if wallet_cash[w] < 0 and wallet_cash[w] > -1e-9:\n",
    "            wallet_cash[w] = 0.0\n",
    "\n",
    "        active_wallets.add(w)\n",
    "        trade_idx += 1\n",
    "\n",
    "    # recompute value only for wallets that changed in this interval\n",
    "    for w in active_wallets:\n",
    "        j = wallet_to_idx.get(w)\n",
    "        if j is None:\n",
    "            continue\n",
    "        V[k, j] = compute_wallet_value(w, k)\n",
    "\n",
    "    active_wallets.clear()\n",
    "\n",
    "print(\"portfolio value matrix shape:\", V.shape)\n",
    "\n",
    "\n",
    "num_times, num_wallets = V.shape\n",
    "\n",
    "# normalize the wallets\n",
    "V_unit = V.astype(np.float64).copy()\n",
    "\n",
    "for j in range(num_wallets):\n",
    "    col = V[:, j]\n",
    "    # find the first strictly positive value as the starting capital\n",
    "    positive_idx = np.where(col > 0)[0]\n",
    "    if len(positive_idx) == 0:\n",
    "        # wallet is always zero\n",
    "        continue\n",
    "    first_idx = positive_idx[0]\n",
    "    start_val = col[first_idx]\n",
    "    if start_val <= 0:\n",
    "        continue\n",
    "    # normalize the whole path by start_val\n",
    "    V_unit[:, j] = col / start_val\n",
    "\n",
    "# get the percentage changes\n",
    "R_wallet = np.zeros((num_times, num_wallets), dtype=np.float64)\n",
    "V_prev = V_unit[:-1, :]\n",
    "V_curr = V_unit[1:, :]\n",
    "\n",
    "mask = V_prev > 0\n",
    "delta = V_curr - V_prev\n",
    "R_step = np.zeros_like(V_prev)\n",
    "np.divide(delta, V_prev, out=R_step, where=mask)\n",
    "\n",
    "R_wallet[1:, :] = R_step\n",
    "\n",
    "# get the historical sharpe\n",
    "r = R_wallet[1:, :]\n",
    "mean_r = r.mean(axis=0)\n",
    "std_r = r.std(axis=0)\n",
    "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "    sharpe_vec = np.where(std_r > 0, mean_r / std_r, 0.0)\n",
    "\n",
    "wallet_sharpe = {wallet_list[j]: float(sharpe_vec[j]) for j in range(num_wallets)}\n",
    "\n",
    "print(\"computed historical sharpe for\", len(wallet_sharpe), \"wallets\")\n",
    "print(\"example wallet sharpe (first 10):\")\n",
    "print(list(wallet_sharpe.items())[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208c569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not wallet_list:\n",
    "    raise ValueError(\"wallet_list is empty; run the value computation cell first.\")\n",
    "\n",
    "# get wallets that we are interested in\n",
    "def select_wallets_with_sell_and_final_cash(df_trades_wallet, wallet_list, N=9):\n",
    "    \n",
    "    # the ret list\n",
    "    found_wallets = []\n",
    "    candidates = np.random.permutation(wallet_list)\n",
    "    \n",
    "    # iterate over the candidates\n",
    "    for w in candidates:\n",
    "        if len(found_wallets) >= N:\n",
    "            break\n",
    "\n",
    "\n",
    "        # get the trades for the wallet\n",
    "        df_w = df_trades_wallet[df_trades_wallet[\"proxy_wallet\"] == w]\n",
    "        sides = set(df_w[\"side\"]) if not df_w.empty else set()\n",
    "        if \"SELL\" in sides and \"BUY\" in sides:\n",
    "            # simulate to determine final cash\n",
    "            cash = float(wallet_initial_cash.get(w, 0.0))\n",
    "            pos = dict(wallet_initial_shares.get(w, {}))\n",
    "\n",
    "\n",
    "            # get the data in the right format\n",
    "            time_arr_w = df_w[\"trade_time\"].to_numpy()\n",
    "            asset_arr_w = df_w[\"asset\"].to_numpy()\n",
    "            side_arr_w = df_w[\"side\"].to_numpy()\n",
    "            size_arr_w = df_w[\"size\"].to_numpy(dtype=float)\n",
    "            price_arr_w = df_w[\"price\"].to_numpy(dtype=float)\n",
    "            n_trades_w = len(df_w)\n",
    "\n",
    "\n",
    "            # iterate over the grid times\n",
    "            trade_idx = 0\n",
    "            for k, t in enumerate(grid_times):\n",
    "                while trade_idx < n_trades_w and time_arr_w[trade_idx] <= t:\n",
    "                    a = asset_arr_w[trade_idx]\n",
    "                    s = side_arr_w[trade_idx]\n",
    "                    q = size_arr_w[trade_idx]\n",
    "                    p = price_arr_w[trade_idx]\n",
    "\n",
    "                    if a not in pos:\n",
    "                        pos[a] = 0.0\n",
    "\n",
    "                    if s == \"BUY\":\n",
    "                        pos[a] += q\n",
    "                        cash -= q * p\n",
    "                    else:\n",
    "                        pos[a] -= q\n",
    "                        cash += q * p\n",
    "\n",
    "                    if pos[a] < 0 and pos[a] > -1e-9:\n",
    "                        pos[a] = 0.0\n",
    "                    if cash < 0 and cash > -1e-9:\n",
    "                        cash = 0.0\n",
    "\n",
    "                    trade_idx += 1\n",
    "            # check final cash\n",
    "            if cash > 0:\n",
    "                found_wallets.append(w)\n",
    "    if len(found_wallets) < N:\n",
    "        print(f\"Only found {len(found_wallets)} wallets with both BUY and SELL and final cash > 0.\")\n",
    "    return found_wallets\n",
    "\n",
    "wallets_selected = select_wallets_with_sell_and_final_cash(df_trades_wallet, wallet_list, N=9)\n",
    "print(f\"Selected {len(wallets_selected)} wallets for visualization:\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 14))\n",
    "axes = axes.flat if hasattr(axes, 'flat') else axes.reshape(-1)\n",
    "\n",
    "for i, wallet_selected in enumerate(wallets_selected):\n",
    "    ax = axes[i]\n",
    "    df_w = df_trades_wallet[df_trades_wallet[\"proxy_wallet\"] == wallet_selected].copy()\n",
    "\n",
    "    if \"trade_time\" in df_w.columns:\n",
    "        df_w = df_w.sort_values(\"trade_time\").reset_index(drop=True)\n",
    "    else:\n",
    "        raise KeyError(\"Column 'trade_time' does not exist in df_trades_wallet, cannot proceed.\")\n",
    "\n",
    "    if df_w.empty:\n",
    "        print(f\"no trades found for wallet {wallet_selected}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    cash = float(wallet_initial_cash.get(wallet_selected, 0.0))\n",
    "    pos = dict(wallet_initial_shares.get(wallet_selected, {}))\n",
    "\n",
    "\n",
    "    # get the data in the right format\n",
    "    time_arr_w = df_w[\"trade_time\"].to_numpy()\n",
    "    asset_arr_w = df_w[\"asset\"].to_numpy()\n",
    "    side_arr_w = df_w[\"side\"].to_numpy()\n",
    "    size_arr_w = df_w[\"size\"].to_numpy(dtype=float)\n",
    "    price_arr_w = df_w[\"price\"].to_numpy(dtype=float)\n",
    "    n_trades_w = len(df_w)\n",
    "\n",
    "    cash_series = np.zeros(len(grid_times), dtype=np.float64)\n",
    "    pos_value_series = np.zeros(len(grid_times), dtype=np.float64)\n",
    "    total_series = np.zeros(len(grid_times), dtype=np.float64)\n",
    "\n",
    "    trade_idx = 0\n",
    "\n",
    "\n",
    "    # check each of the grid times  \n",
    "    for k, t in enumerate(grid_times):\n",
    "        while trade_idx < n_trades_w and time_arr_w[trade_idx] <= t:\n",
    "            a = asset_arr_w[trade_idx]\n",
    "            s = side_arr_w[trade_idx]\n",
    "            q = size_arr_w[trade_idx]\n",
    "            p = price_arr_w[trade_idx]\n",
    "\n",
    "            if a not in pos:\n",
    "                pos[a] = 0.0\n",
    "\n",
    "            if s == \"BUY\":\n",
    "                pos[a] += q\n",
    "                cash -= q * p\n",
    "            else:\n",
    "                pos[a] -= q\n",
    "                cash += q * p\n",
    "\n",
    "            if pos[a] < 0 and pos[a] > -1e-9:\n",
    "                pos[a] = 0.0\n",
    "            if cash < 0 and cash > -1e-9:\n",
    "                cash = 0.0\n",
    "\n",
    "            trade_idx += 1\n",
    "\n",
    "        pos_val = 0.0\n",
    "        for a, q in pos.items():\n",
    "            if q == 0.0:\n",
    "                continue\n",
    "            tok_idx = token_to_idx.get(a)\n",
    "            if tok_idx is None:\n",
    "                continue\n",
    "            p_t = price_grid[tok_idx, k]\n",
    "            pos_val += q * p_t\n",
    "\n",
    "        val = cash + pos_val\n",
    "\n",
    "        cash_series[k] = cash\n",
    "        pos_value_series[k] = pos_val\n",
    "        total_series[k] = val\n",
    "    \n",
    "    # show me the wlalets\n",
    "    ax.plot(grid_times, total_series, label=\"total value\")\n",
    "    ax.plot(grid_times, cash_series, label=\"cash\")\n",
    "    ax.plot(grid_times, pos_value_series, label=\"positions value\")\n",
    "    ax.set_xlabel(\"time\")\n",
    "    ax.set_ylabel(\"value\")\n",
    "    ax.set_title(f\"wallet {wallet_selected[:8]}...\")\n",
    "    ax.legend(fontsize=\"small\")\n",
    "    ax.grid(True, alpha=0.25)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(wallets_selected), 9):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "fig.suptitle(\"Portfolio evolution for example wallets\", fontsize=18)\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f2208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose how many chunks you want\n",
    "NUM_CHUNKS = 6\n",
    "\n",
    "\n",
    "# the number of times that we have data for\n",
    "num_times = len(grid_times)\n",
    "if NUM_CHUNKS < 4:\n",
    "    raise ValueError(\"NUM_CHUNKS must be at least 4 (need train, val, test).\")\n",
    "\n",
    "chunk_edges = np.linspace(0, num_times, NUM_CHUNKS + 1, dtype=int)\n",
    "\n",
    "def get_chunk_indices(k):\n",
    "    if not (1 <= k <= NUM_CHUNKS):\n",
    "        raise ValueError(f\"chunk index k must be in 1..{NUM_CHUNKS}\")\n",
    "    return int(chunk_edges[k - 1]), int(chunk_edges[k])\n",
    "\n",
    "print(\"number of grid_times:\", num_times)\n",
    "for k in range(1, NUM_CHUNKS + 1):\n",
    "    s, e = get_chunk_indices(k)\n",
    "    print(f\"chunk {k}: indices [{s}, {e}) length {e - s}\")\n",
    "\n",
    "# we will build features and graph using only data up to the end of chunk\n",
    "cut_start, cut_end = get_chunk_indices(NUM_CHUNKS - 2)\n",
    "cut_idx = cut_end - 1\n",
    "t_cut = grid_times[cut_idx]\n",
    "print(f\"feature/graph cutoff t_cut (end of chunk {NUM_CHUNKS-2}):\", t_cut)\n",
    "\n",
    "# feature flags\n",
    "feature_flags = {\n",
    "    \"use_return_mean_std\": True,\n",
    "    \"use_num_series\": True,\n",
    "    \"use_num_events\": True,\n",
    "    \"use_num_markets\": True,\n",
    "    \"use_log_trades_per_hour\": True,\n",
    "    \"use_avg_trade_size\": True,\n",
    "    \"use_avg_trade_price\": True,\n",
    "    \"use_max_drawdown_pct\": True,\n",
    "    \"use_series_onehot\": True,\n",
    "    \"use_degree_in\": True,\n",
    "    \"use_degree_out\": True,\n",
    "    \"use_orc_curvature\": False,\n",
    "    \"use_local_curvature\": True\n",
    "}\n",
    "\n",
    "# onehot dimension for most traded series\n",
    "TOP_SERIES_K = 20  \n",
    "\n",
    "r_up_to_cut = R_wallet[1:cut_end, :]\n",
    "\n",
    "ret_mean_full = r_up_to_cut.mean(axis=0).astype(np.float32)\n",
    "ret_std_full = r_up_to_cut.std(axis=0).astype(np.float32)\n",
    "\n",
    "df_trades_tcut = df_trades[df_trades[\"trade_time\"] <= t_cut].copy()\n",
    "df_wallet_tcut = df_trades_wallet[df_trades_wallet[\"trade_time\"] <= t_cut].copy()\n",
    "\n",
    "df_trades_tcut = df_trades_tcut[df_trades_tcut[\"proxy_wallet\"].isin(wallet_list)]\n",
    "df_wallet_tcut = df_wallet_tcut[df_wallet_tcut[\"proxy_wallet\"].isin(wallet_list)]\n",
    "\n",
    "def reindex_to_wallets(series, default=0.0, dtype=np.float32):\n",
    "    s = series.reindex(wallet_list).fillna(default).astype(dtype)\n",
    "    return s.to_numpy()\n",
    "\n",
    "num_series_per_wallet = reindex_to_wallets(\n",
    "    df_trades_tcut.groupby(\"proxy_wallet\")[\"series_id\"].nunique()\n",
    ")\n",
    "\n",
    "num_events_per_wallet = reindex_to_wallets(\n",
    "    df_trades_tcut.groupby(\"proxy_wallet\")[\"event_id\"].nunique()\n",
    ")\n",
    "\n",
    "num_markets_per_wallet = reindex_to_wallets(\n",
    "    df_trades_tcut.groupby(\"proxy_wallet\")[\"market_id\"].nunique()\n",
    ")\n",
    "\n",
    "if not df_wallet_tcut.empty:\n",
    "    agg_time = df_wallet_tcut.groupby(\"proxy_wallet\")[\"trade_time\"].agg([\"count\", \"min\", \"max\"])\n",
    "    horizon_hours = (agg_time[\"max\"] - agg_time[\"min\"]).dt.total_seconds() / 3600.0\n",
    "    horizon_hours = horizon_hours.replace(0, 1.0 / 3600.0)\n",
    "    trades_per_hour = agg_time[\"count\"] / horizon_hours\n",
    "    log_trades_per_hour = np.log1p(trades_per_hour)\n",
    "else:\n",
    "    log_trades_per_hour = pd.Series([], dtype=float)\n",
    "\n",
    "log_trades_per_wallet = reindex_to_wallets(log_trades_per_hour)\n",
    "\n",
    "avg_size_per_wallet = reindex_to_wallets(\n",
    "    df_wallet_tcut.groupby(\"proxy_wallet\")[\"size\"].mean()\n",
    ")\n",
    "\n",
    "avg_price_per_wallet = reindex_to_wallets(\n",
    "    df_wallet_tcut.groupby(\"proxy_wallet\")[\"price\"].mean()\n",
    ")\n",
    "\n",
    "max_drawdown_pct = np.zeros(num_wallets, dtype=np.float32)\n",
    "\n",
    "for j in range(num_wallets):\n",
    "    # up to t_cut\n",
    "    v = V_unit[:cut_end, j]  \n",
    "    if not np.any(v > 0):\n",
    "        max_drawdown_pct[j] = 0.0\n",
    "        continue\n",
    "    running_max = np.maximum.accumulate(v)\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        dd = np.where(running_max > 0, v / running_max - 1.0, 0.0)\n",
    "    min_dd = dd.min()\n",
    "    max_drawdown_pct[j] = float(-min_dd) if min_dd < 0 else 0.0\n",
    "\n",
    "series_counts = (\n",
    "    df_trades_tcut[\"series_id\"]\n",
    "    .dropna()\n",
    "    .value_counts()\n",
    "    .head(TOP_SERIES_K)\n",
    ")\n",
    "top_series_ids = list(series_counts.index)\n",
    "series_id_to_col = {sid: idx for idx, sid in enumerate(top_series_ids)}\n",
    "\n",
    "series_onehot = np.zeros((num_wallets, len(top_series_ids)), dtype=np.float32)\n",
    "\n",
    "if len(top_series_ids) > 0:\n",
    "    df_series_pairs = (\n",
    "        df_trades_tcut[[\"proxy_wallet\", \"series_id\"]]\n",
    "        .dropna(subset=[\"series_id\"])\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    for wallet, sid in df_series_pairs.itertuples(index=False):\n",
    "        if sid in series_id_to_col and wallet in wallet_to_idx:\n",
    "            wi = wallet_to_idx[wallet]\n",
    "            si = series_id_to_col[sid]\n",
    "            series_onehot[wi, si] = 1.0\n",
    "\n",
    "print(\"top series used for one-hot:\", top_series_ids)\n",
    "\n",
    "feature_list = []\n",
    "\n",
    "if feature_flags[\"use_return_mean_std\"]:\n",
    "    feature_list.append(ret_mean_full[:, None])\n",
    "    feature_list.append(ret_std_full[:, None])\n",
    "\n",
    "if feature_flags[\"use_num_series\"]:\n",
    "    feature_list.append(num_series_per_wallet[:, None])\n",
    "\n",
    "if feature_flags[\"use_num_events\"]:\n",
    "    feature_list.append(num_events_per_wallet[:, None])\n",
    "\n",
    "if feature_flags[\"use_num_markets\"]:\n",
    "    feature_list.append(num_markets_per_wallet[:, None])\n",
    "\n",
    "if feature_flags[\"use_log_trades_per_hour\"]:\n",
    "    feature_list.append(log_trades_per_wallet[:, None])\n",
    "\n",
    "if feature_flags[\"use_avg_trade_size\"]:\n",
    "    feature_list.append(avg_size_per_wallet[:, None])\n",
    "\n",
    "if feature_flags[\"use_avg_trade_price\"]:\n",
    "    feature_list.append(avg_price_per_wallet[:, None])\n",
    "\n",
    "if feature_flags[\"use_max_drawdown_pct\"]:\n",
    "    feature_list.append(max_drawdown_pct[:, None])\n",
    "\n",
    "if feature_flags[\"use_series_onehot\"] and series_onehot.shape[1] > 0:\n",
    "    feature_list.append(series_onehot)\n",
    "\n",
    "X_base = np.concatenate(feature_list, axis=1).astype(np.float32)\n",
    "print(\"X_base shape:\", X_base.shape)\n",
    "print(\"feature_flags:\", feature_flags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377b19a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph = df_trades[\n",
    "    (df_trades[\"trade_time\"] <= t_cut) &\n",
    "    (df_trades[\"proxy_wallet\"].isin(wallet_list))\n",
    "].copy()\n",
    "\n",
    "df_graph = df_graph.sort_values(\"trade_time\").reset_index(drop=True)\n",
    "\n",
    "cols_needed = [\n",
    "    \"id\", \"transaction_hash\", \"condition_id\", \"size\", \"price\",\n",
    "    \"timestamp\", \"trade_time\", \"side\", \"proxy_wallet\",\n",
    "    \"asset\", \"outcome_id\", \"outcome_index\", \"trade_type_id\",\n",
    "    \"created_at\", \"updated_at\"\n",
    "]\n",
    "for c in cols_needed:\n",
    "    if c not in df_graph.columns:\n",
    "        raise ValueError(f\"missing column {c} in df_graph\")\n",
    "\n",
    "all_trades_graph = list(df_graph[cols_needed].itertuples(index=False, name=None))\n",
    "\n",
    "IDX_ID = 0\n",
    "IDX_TX_HASH = 1\n",
    "IDX_CONDITION_ID = 2\n",
    "IDX_SIZE = 3\n",
    "IDX_PRICE = 4\n",
    "IDX_TIMESTAMP_INT = 5\n",
    "IDX_TRADE_TIME   = 6\n",
    "IDX_SIDE = 7\n",
    "IDX_PROXY_WALLET = 8\n",
    "IDX_ASSET = 9\n",
    "IDX_OUTCOME_ID = 10\n",
    "IDX_OUTCOME_INDEX = 11\n",
    "IDX_TRADE_TYPE_ID = 12\n",
    "IDX_CREATED_AT = 13\n",
    "IDX_UPDATED_AT = 14\n",
    "\n",
    "MIN_RELATIONSHIP_WEIGHT = 0.65\n",
    "MAX_GAP_SECONDS = 15\n",
    "CO_TRADE_EXTENSION_SECONDS = 40\n",
    "\n",
    "sorted_trades = sorted(all_trades_graph, key=lambda r: r[IDX_TRADE_TIME])\n",
    "\n",
    "trades_by_cp = defaultdict(list)\n",
    "for row in tqdm(sorted_trades, desc=\"populating trades_by_cp\", disable=len(sorted_trades) < 10000):\n",
    "    condition_id = row[IDX_CONDITION_ID]\n",
    "    proxy_wallet = row[IDX_PROXY_WALLET]\n",
    "    trades_by_cp[(condition_id, proxy_wallet)].append(row)\n",
    "\n",
    "continuum_windows = []\n",
    "\n",
    "for (condition_id, proxy_wallet), rows_cp in tqdm(\n",
    "    trades_by_cp.items(),\n",
    "    desc=\"building continuum windows\",\n",
    "    disable=len(trades_by_cp) < 5000\n",
    "):\n",
    "    if not rows_cp:\n",
    "        continue\n",
    "    current_start = rows_cp[0][IDX_TRADE_TIME]\n",
    "    current_end = rows_cp[0][IDX_TRADE_TIME]\n",
    "\n",
    "    for row in rows_cp[1:]:\n",
    "        ts = row[IDX_TRADE_TIME]\n",
    "        if (ts - current_end).total_seconds() <= MAX_GAP_SECONDS:\n",
    "            current_end = ts\n",
    "        else:\n",
    "            continuum_windows.append(\n",
    "                {\"condition_id\": condition_id, \"wallet\": proxy_wallet,\n",
    "                 \"min_ts\": current_start, \"max_ts\": current_end}\n",
    "            )\n",
    "            current_start = ts\n",
    "            current_end = ts\n",
    "\n",
    "    continuum_windows.append(\n",
    "        {\"condition_id\": condition_id, \"wallet\": proxy_wallet,\n",
    "         \"min_ts\": current_start, \"max_ts\": current_end}\n",
    "    )\n",
    "\n",
    "print(\"number of continuum windows:\", len(continuum_windows))\n",
    "\n",
    "trades_by_condition = defaultdict(list)\n",
    "for row in tqdm(sorted_trades, desc=\"assigning trades by condition\", disable=len(sorted_trades) < 10000):\n",
    "    trades_by_condition[row[IDX_CONDITION_ID]].append(row)\n",
    "\n",
    "cond_times = {}\n",
    "for cond_id, rows_c in trades_by_condition.items():\n",
    "    cond_times[cond_id] = [r[IDX_TRADE_TIME] for r in rows_c]\n",
    "\n",
    "total_windows_by_wallet = defaultdict(int)\n",
    "co_trade_windows = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for w in tqdm(continuum_windows, desc=\"finding co-trade windows\", disable=len(continuum_windows) < 5000):\n",
    "    cond_id = w[\"condition_id\"]\n",
    "    wallet_a = w[\"wallet\"]\n",
    "    min_ts = w[\"min_ts\"]\n",
    "    max_ts = w[\"max_ts\"]\n",
    "\n",
    "    start_ts = min_ts\n",
    "    end_ts = max_ts + timedelta(seconds=CO_TRADE_EXTENSION_SECONDS)\n",
    "\n",
    "    total_windows_by_wallet[wallet_a] += 1\n",
    "\n",
    "    rows_c = trades_by_condition[cond_id]\n",
    "    times_c = cond_times[cond_id]\n",
    "\n",
    "    left = bisect_left(times_c, start_ts)\n",
    "    right = bisect_right(times_c, end_ts)\n",
    "\n",
    "    co_wallets = set()\n",
    "    for row in rows_c[left:right]:\n",
    "        wallet_b = row[IDX_PROXY_WALLET]\n",
    "        if wallet_b != wallet_a:\n",
    "            co_wallets.add(wallet_b)\n",
    "\n",
    "    for wallet_b in co_wallets:\n",
    "        co_trade_windows[wallet_a][wallet_b] += 1\n",
    "\n",
    "directed_graph = defaultdict(dict)\n",
    "\n",
    "for wallet_a, targets in tqdm(co_trade_windows.items(), desc=\"building directed graph\", disable=len(co_trade_windows) < 3000):\n",
    "    if wallet_a not in wallet_to_idx:\n",
    "        continue\n",
    "    total_windows = total_windows_by_wallet[wallet_a]\n",
    "    if total_windows == 0:\n",
    "        continue\n",
    "    for wallet_b, count in targets.items():\n",
    "        if wallet_b not in wallet_to_idx:\n",
    "            continue\n",
    "        pct_overlap = count / total_windows\n",
    "        if pct_overlap >= MIN_RELATIONSHIP_WEIGHT:\n",
    "            directed_graph[wallet_a][wallet_b] = float(pct_overlap)\n",
    "\n",
    "print(\"wallets with outgoing edges:\", len(directed_graph))\n",
    "\n",
    "\n",
    "print(\"computing wallet degrees\")\n",
    "print(\"num_wallets:\", num_wallets)\n",
    "deg_out = np.zeros(num_wallets, dtype=np.float32)\n",
    "deg_in = np.zeros(num_wallets, dtype=np.float32)\n",
    "\n",
    "for w_a, neighbors in tqdm(directed_graph.items(), desc=\"computing wallet degrees\", disable=len(directed_graph) < 3000):\n",
    "    if w_a not in wallet_to_idx:\n",
    "        continue\n",
    "    i = wallet_to_idx[w_a]\n",
    "    deg_out[i] = len(neighbors)\n",
    "    for w_b in neighbors.keys():\n",
    "        if w_b in wallet_to_idx:\n",
    "            j = wallet_to_idx[w_b]\n",
    "            deg_in[j] += 1.0\n",
    "\n",
    "orc_vec = np.zeros(num_wallets, dtype=np.float32)\n",
    "orc_feature_index = None\n",
    "\n",
    "if feature_flags.get(\"use_orc_curvature\", False):\n",
    "    try:\n",
    "        import networkx as nx\n",
    "        from GraphRicciCurvature.OllivierRicci import OllivierRicci\n",
    "\n",
    "        G_orc = nx.Graph()\n",
    "        for w_a, neighbors in tqdm(\n",
    "            directed_graph.items(),\n",
    "            desc=\"building undirected graph for ORC\",\n",
    "            disable=len(directed_graph) < 3000\n",
    "        ):\n",
    "            for w_b, weight in neighbors.items():\n",
    "                if w_a == w_b:\n",
    "                    continue\n",
    "                w_val = float(weight)\n",
    "                if G_orc.has_edge(w_a, w_b):\n",
    "                    G_orc[w_a][w_b][\"weight\"] = 0.5 * (G_orc[w_a][w_b][\"weight\"] + w_val)\n",
    "                else:\n",
    "                    G_orc.add_edge(w_a, w_b, weight=w_val)\n",
    "\n",
    "        print(\"graph for ORC: nodes\", G_orc.number_of_nodes(), \"edges\", G_orc.number_of_edges())\n",
    "\n",
    "        if G_orc.number_of_edges() > 0:\n",
    "            orc = OllivierRicci(G_orc, alpha=0.5, verbose=\"INFO\")\n",
    "            orc.compute_ricci_curvature()\n",
    "\n",
    "            node_orc = {n: data.get(\"ricciCurvature\", 0.0) for n, data in G_orc.nodes(data=True)}\n",
    "            for w, idx in tqdm(wallet_to_idx.items(), desc=\"mapping ORC values to wallets\", disable=len(wallet_to_idx) < 2000):\n",
    "                orc_vec[idx] = float(node_orc.get(w, 0.0))\n",
    "\n",
    "            print(\n",
    "                \"ollivier-ricci curvature stats:\",\n",
    "                \"min\", float(orc_vec.min()),\n",
    "                \"max\", float(orc_vec.max()),\n",
    "                \"mean\", float(orc_vec.mean())\n",
    "            )\n",
    "        else:\n",
    "            print(\"empty graph for ORC, leaving orc_vec as zeros.\")\n",
    "    except Exception as e:\n",
    "        print(\"ORC curvature computation failed, skipping. error:\", e)\n",
    "\n",
    "\n",
    "local_curv_vec = np.zeros(num_wallets, dtype=np.float32)\n",
    "local_curv_feature_index = None\n",
    "\n",
    "\n",
    "\n",
    "# good approximation of the local curvature\n",
    "if feature_flags.get(\"use_local_curvature\", False):\n",
    "    try:\n",
    "\n",
    "        G_curv = nx.Graph()\n",
    "        for w_a, neighbors in tqdm(\n",
    "            directed_graph.items(),\n",
    "            desc=\"building undirected graph for local curvature\",\n",
    "            disable=len(directed_graph) < 3000\n",
    "        ):\n",
    "            for w_b, weight in neighbors.items():\n",
    "                if w_a == w_b:\n",
    "                    continue\n",
    "                w_val = float(weight)\n",
    "                if G_curv.has_edge(w_a, w_b):\n",
    "                    # keep max weight (or average; not critical)\n",
    "                    G_curv[w_a][w_b][\"weight\"] = max(G_curv[w_a][w_b][\"weight\"], w_val)\n",
    "                else:\n",
    "                    G_curv.add_edge(w_a, w_b, weight=w_val)\n",
    "\n",
    "        print(\n",
    "            \"graph for local curvature: nodes\",\n",
    "            G_curv.number_of_nodes(),\n",
    "            \"edges\",\n",
    "            G_curv.number_of_edges()\n",
    "        )\n",
    "\n",
    "        if G_curv.number_of_edges() > 0:\n",
    "            clustering = nx.clustering(G_curv, weight=\"weight\")\n",
    "\n",
    "            # curvature-like scalar: kappa_i = 1 - C_i\n",
    "            for w, idx in wallet_to_idx.items():\n",
    "                C_i = float(clustering.get(w, 0.0))\n",
    "                local_curv_vec[idx] = 1.0 - C_i\n",
    "\n",
    "            print(\n",
    "                \"local curvature proxy stats:\",\n",
    "                \"min\", float(local_curv_vec.min()),\n",
    "                \"max\", float(local_curv_vec.max()),\n",
    "                \"mean\", float(local_curv_vec.mean())\n",
    "            )\n",
    "        else:\n",
    "            print(\"empty graph for local curvature, leaving local_curv_vec as zeros.\")\n",
    "    except Exception as e:\n",
    "        print(\"local curvature computation failed, skipping. error:\", e)\n",
    "\n",
    "\n",
    "X_all = X_base.astype(np.float32)\n",
    "X_all_aug = X_all\n",
    "\n",
    "if feature_flags.get(\"use_degree_out\", True):\n",
    "    X_all_aug = np.concatenate([X_all_aug, deg_out[:, None]], axis=1)\n",
    "\n",
    "if feature_flags.get(\"use_degree_in\", True):\n",
    "    X_all_aug = np.concatenate([X_all_aug, deg_in[:, None]], axis=1)\n",
    "\n",
    "if feature_flags.get(\"use_orc_curvature\", False):\n",
    "    X_all_aug = np.concatenate([X_all_aug, orc_vec[:, None]], axis=1)\n",
    "    orc_feature_index = X_all_aug.shape[1] - 1\n",
    "    print(\"ORC feature column index:\", orc_feature_index)\n",
    "else:\n",
    "    orc_feature_index = None\n",
    "\n",
    "if feature_flags.get(\"use_local_curvature\", False):\n",
    "    X_all_aug = np.concatenate([X_all_aug, local_curv_vec[:, None]], axis=1)\n",
    "    local_curv_feature_index = X_all_aug.shape[1] - 1\n",
    "    print(\"local curvature feature column index:\", local_curv_feature_index)\n",
    "else:\n",
    "    local_curv_feature_index = None\n",
    "\n",
    "print(\"X_all_aug shape:\", X_all_aug.shape)\n",
    "\n",
    "# record which columns are curvature columns in X_all_aug\n",
    "curvature_col_indices = []\n",
    "\n",
    "if orc_feature_index is not None:\n",
    "    curvature_col_indices.append(orc_feature_index)\n",
    "\n",
    "if local_curv_feature_index is not None:\n",
    "    curvature_col_indices.append(local_curv_feature_index)\n",
    "\n",
    "curvature_col_indices = sorted(curvature_col_indices)\n",
    "\n",
    "print(\"curvature columns:\", curvature_col_indices)\n",
    "\n",
    "edge_src = []\n",
    "edge_dst = []\n",
    "edge_weight_list = []\n",
    "\n",
    "for w_a, neighbors in tqdm(directed_graph.items(), desc=\"building edge indices and weights\", disable=len(directed_graph) < 3000):\n",
    "    i = wallet_to_idx[w_a]\n",
    "    for w_b, weight in neighbors.items():\n",
    "        j = wallet_to_idx[w_b]\n",
    "        edge_src.append(i)\n",
    "        edge_dst.append(j)\n",
    "        edge_weight_list.append(float(weight))\n",
    "\n",
    "if len(edge_src) == 0:\n",
    "    edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "    edge_attr = torch.empty((0, 1), dtype=torch.float32)\n",
    "else:\n",
    "    edge_index = torch.tensor([edge_src, edge_dst], dtype=torch.long)\n",
    "    edge_attr = torch.tensor(edge_weight_list, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "print(\"edge_index shape:\", edge_index.shape)\n",
    "print(\"edge_attr shape:\", edge_attr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using device:\", device)\n",
    "\n",
    "deg_total = deg_in + deg_out\n",
    "active_mask = deg_total >= 1\n",
    "active_idx = np.where(active_mask)[0]\n",
    "\n",
    "print(f\"active wallets: {active_idx.size} / {num_wallets}\")\n",
    "\n",
    "num_features = X_all_aug.shape[1]\n",
    "\n",
    "if \"curvature_col_indices\" not in globals():\n",
    "    curvature_col_indices = []\n",
    "\n",
    "all_cols = np.arange(num_features)\n",
    "curvature_col_indices = np.array(curvature_col_indices, dtype=int) if len(curvature_col_indices) > 0 else np.array([], dtype=int)\n",
    "\n",
    "if curvature_col_indices.size == 0:\n",
    "    print(\"no curvature columns detected\")\n",
    "    non_curv_cols = all_cols\n",
    "else:\n",
    "    mask_cols = np.ones(num_features, dtype=bool)\n",
    "    mask_cols[curvature_col_indices] = False\n",
    "    non_curv_cols = all_cols[mask_cols]\n",
    "\n",
    "X_all_no_curv_full = X_all_aug[:, non_curv_cols].astype(np.float32)\n",
    "X_all_with_curv_full = X_all_aug.astype(np.float32)\n",
    "\n",
    "print(\"X_all_no_curv_full shape:\", X_all_no_curv_full.shape)\n",
    "print(\"X_all_with_curv_full shape:\", X_all_with_curv_full.shape)\n",
    "\n",
    "# restrict to active wallets\n",
    "X_features_no_curv = X_all_no_curv_full[active_idx]\n",
    "X_features_with_curv = X_all_with_curv_full[active_idx]\n",
    "\n",
    "# compress returns to active wallets\n",
    "R_wallet_active = R_wallet[:, active_idx].astype(np.float32)\n",
    "\n",
    "# build compressed edge_index induced by active_idx\n",
    "edge_index_np = edge_index.cpu().numpy()\n",
    "src_full = edge_index_np[0]\n",
    "dst_full = edge_index_np[1]\n",
    "\n",
    "\n",
    "# the edge_index_active is the edge_index of the active wallets\n",
    "edge_keep_mask = active_mask[src_full] & active_mask[dst_full]\n",
    "src_kept = src_full[edge_keep_mask]\n",
    "dst_kept = dst_full[edge_keep_mask]\n",
    "\n",
    "idx_map = -np.ones(num_wallets, dtype=np.int64)\n",
    "idx_map[active_idx] = np.arange(active_idx.size, dtype=np.int64)\n",
    "\n",
    "src_sub = idx_map[src_kept]\n",
    "dst_sub = idx_map[dst_kept]\n",
    "\n",
    "edge_index_active = torch.tensor(\n",
    "    np.vstack([src_sub, dst_sub]),\n",
    "    dtype=torch.long,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"compressed edge_index shape:\", edge_index_active.shape)\n",
    "\n",
    "\n",
    "def normalize_long_short(scores, eps=1e-8):\n",
    "    w = torch.tanh(scores)\n",
    "    w = w - w.mean()\n",
    "    sum_abs = w.abs().sum()\n",
    "    sum_abs = torch.where(\n",
    "        sum_abs < eps,\n",
    "        torch.tensor(1.0, device=w.device, dtype=w.dtype),\n",
    "        sum_abs\n",
    "    )\n",
    "    w = 2.0 * w / sum_abs\n",
    "    return w\n",
    "\n",
    "class GNNPortfolioModel(nn.Module):\n",
    "    def __init__(self, in_dim=2, hidden_dim=16, num_layers=3):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        \n",
    "        if num_layers == 1:\n",
    "            self.convs.append(SAGEConv(in_dim, hidden_dim))\n",
    "        \n",
    "        else:\n",
    "            self.convs.append(SAGEConv(in_dim, hidden_dim))\n",
    "            for _ in range(num_layers - 2):\n",
    "                self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
    "            self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
    "        \n",
    "        self.score_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = x\n",
    "        for conv in self.convs:\n",
    "            h = conv(h, edge_index)\n",
    "            h = F.relu(h)\n",
    "        scores = self.score_mlp(h).squeeze(-1)\n",
    "        retweights = normalize_long_short(scores)\n",
    "        return retweights, scores\n",
    "\n",
    "def sharpe_loss(weights, R_steps, eps=1e-8):\n",
    "    port_ret = torch.matmul(R_steps, weights)\n",
    "    mean_ret = port_ret.mean()\n",
    "    std_ret = port_ret.std(unbiased=False) + eps\n",
    "    sharpe = mean_ret / std_ret\n",
    "    loss = -sharpe\n",
    "    return loss, sharpe, mean_ret, std_ret\n",
    "\n",
    "\n",
    "def get_return_block(k):\n",
    "    s, e = get_chunk_indices(k)\n",
    "    if e <= s + 1:\n",
    "        raise ValueError(f\"chunk {k} too short for returns: start {s}, end {e}\")\n",
    "    return R_wallet_active[s + 1:e, :].astype(np.float32)\n",
    "\n",
    "def run_experiment(X_np, R_train_np, R_val_np, label):\n",
    "    print(\"experiment:\", label)\n",
    "\n",
    "\n",
    "    # get the data in the right format\n",
    "    x_all = torch.from_numpy(X_np).float().to(device)\n",
    "    edge_index_local = edge_index_active\n",
    "    R_train_local = torch.from_numpy(R_train_np).float().to(device)\n",
    "    R_val_local = torch.from_numpy(R_val_np).float().to(device)\n",
    "\n",
    "    in_dim = x_all.size(1)\n",
    "    model = GNNPortfolioModel(in_dim=in_dim, hidden_dim=16, num_layers=8).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "    num_epochs = 2000\n",
    "    early_stop_patience = 300\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_sharpes = []\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model_state = None\n",
    "    best_epoch = 0\n",
    "    epochs_since_improvement = 0\n",
    "\n",
    "\n",
    "    # iterate through the epochs\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        # get the weights for the training set\n",
    "        weights_train, _ = model(x_all, edge_index_local)\n",
    "        loss_tr, sharpe_tr, mean_tr, std_tr = sharpe_loss(weights_train, R_train_local)\n",
    "\n",
    "        loss_tr.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            weights_val, _ = model(x_all, edge_index_local)\n",
    "            loss_val, sharpe_v, mean_v, std_v = sharpe_loss(weights_val, R_val_local)\n",
    "\n",
    "\n",
    "        # append all of the data together\n",
    "        train_losses.append(loss_tr.item())\n",
    "        val_losses.append(loss_val.item())\n",
    "        val_sharpes.append(sharpe_v.item())\n",
    "\n",
    "\n",
    "        # check if we are done or not\n",
    "        if loss_val.item() < best_val_loss - 1e-6:\n",
    "            best_val_loss = loss_val.item()\n",
    "            best_model_state = model.state_dict()\n",
    "            best_epoch = epoch\n",
    "            epochs_since_improvement = 0\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "\n",
    "\n",
    "        # print the results along the way for updates\n",
    "        if epoch % 20 == 0 or epoch == 1:\n",
    "            print(\n",
    "                f\"epoch {epoch:04d} | \"\n",
    "                f\"train loss {loss_tr.item():12f} | \"\n",
    "                f\"train sharpe {sharpe_tr.item():12f} | \"\n",
    "                f\"val sharpe {sharpe_v.item():12f} | \"\n",
    "                f\"val loss {loss_val.item():12f}\"\n",
    "            )\n",
    "\n",
    "        if epochs_since_improvement >= early_stop_patience:\n",
    "            print(f\"\\nearly stopping at epoch {epoch} (best epoch: {best_epoch})\")\n",
    "            break\n",
    "\n",
    "    if best_val_loss is not None and best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"loaded best model from epoch {best_epoch} with val loss {best_val_loss:}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        weights_val_best, _ = model(x_all, edge_index_local)\n",
    "        _, sharpe_v_best, mean_v_best, std_v_best = sharpe_loss(weights_val_best, R_val_local)\n",
    "\n",
    "    print(f\"final best validation sharpe ({label}): {sharpe_v_best.item():}\")\n",
    "\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"model\": model,\n",
    "        \"weights_val\": weights_val_best.detach().cpu().numpy(),\n",
    "        \"val_sharpe\": sharpe_v_best.item(),\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"val_sharpes\": val_sharpes,\n",
    "    }\n",
    "\n",
    "\n",
    "num_chunks = NUM_CHUNKS\n",
    "if num_chunks < 4:\n",
    "    raise ValueError(\"NUM_CHUNKS must be at least 4 for train/val/test setup or else we do not hvae enough data to work with.\")\n",
    "\n",
    "\n",
    "train_blocks = []\n",
    "for i in range(1, num_chunks - 2):\n",
    "    target_k = i + 1\n",
    "    R_k = get_return_block(target_k)\n",
    "    train_blocks.append(R_k)\n",
    "R_train_all = np.concatenate(train_blocks, axis=0).astype(np.float32)\n",
    "R_val_block = get_return_block(num_chunks - 1)\n",
    "R_test_block = get_return_block(num_chunks)\n",
    "\n",
    "results_no_curv = run_experiment(\n",
    "    X_features_no_curv,\n",
    "    R_train_np=R_train_all,\n",
    "    R_val_np=R_val_block,\n",
    "    label=\"no_curvature_features\"\n",
    ")\n",
    "\n",
    "results_with_curv = run_experiment(\n",
    "    X_features_with_curv,\n",
    "    R_train_np=R_train_all,\n",
    "    R_val_np=R_val_block,\n",
    "    label=\"with_curvature_features\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(results_no_curv[\"train_losses\"], label=\"Train Loss (no curvature)\", color=\"blue\", alpha=0.6)\n",
    "plt.plot(results_no_curv[\"val_losses\"], label=\"Val Loss (no curvature)\", color=\"blue\", linestyle=\"dashed\", alpha=0.7)\n",
    "\n",
    "plt.plot(results_with_curv[\"train_losses\"], label=\"Train Loss (with curvature)\", color=\"orange\", alpha=0.7)\n",
    "plt.plot(results_with_curv[\"val_losses\"], label=\"Val Loss (with curvature)\", color=\"orange\", linestyle=\"dashed\", alpha=0.8)\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (negative Sharpe)\")\n",
    "plt.title(\"Train/Validation Losses for GNN Portfolio: With vs. Without Curvature Features\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# evaluate on the test set\n",
    "def eval_on_test(model, X_np, label):\n",
    "    x_all = torch.from_numpy(X_np).float().to(device)\n",
    "    edge_index_local = edge_index_active\n",
    "    R_test_torch = torch.from_numpy(R_test_block).float().to(device)\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        weights_test, _ = model(x_all, edge_index_local)\n",
    "        _, sharpe_test, mean_test, std_test = sharpe_loss(weights_test, R_test_torch)\n",
    "\n",
    "    print(\n",
    "        f\"test result ({label}): \"\n",
    "        f\"sharpe={sharpe_test.item():} | \"\n",
    "        f\"mean_ret={mean_test.item():.6e} | std_ret={std_test.item():.6e}\"\n",
    "    )\n",
    "    return weights_test.detach().cpu().numpy().reshape(-1), sharpe_test.item()\n",
    "\n",
    "weights_no_curv_active, sharpe_test_no_curv = eval_on_test(\n",
    "    results_no_curv[\"model\"],\n",
    "    X_features_no_curv,\n",
    "    \"no_curvature_features\"\n",
    ")\n",
    "\n",
    "weights_with_curv_active, sharpe_test_with_curv = eval_on_test(\n",
    "    results_with_curv[\"model\"],\n",
    "    X_features_with_curv,\n",
    "    \"with_curvature_features\"\n",
    ")\n",
    "\n",
    "print(f\"val sharpe (no curv): {results_no_curv['val_sharpe']:}\")\n",
    "print(f\"val sharpe (with curv): {results_with_curv['val_sharpe']:}\")\n",
    "print(f\"test sharpe (no curv): {sharpe_test_no_curv:}\")\n",
    "print(f\"test sharpe (with curv): {sharpe_test_with_curv:}\")\n",
    "\n",
    "# expand active weights back to full wallet universe for downstream diagnostics\n",
    "final_weights_no_curv = np.zeros(num_wallets, dtype=np.float32)\n",
    "final_weights_no_curv[active_idx] = weights_no_curv_active\n",
    "\n",
    "final_weights_with_curv = np.zeros(num_wallets, dtype=np.float32)\n",
    "final_weights_with_curv[active_idx] = weights_with_curv_active\n",
    "\n",
    "\n",
    "final_weights_val = final_weights_with_curv.copy()\n",
    "print(\"stored final_weights_val (with curvature) length:\", final_weights_val.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34331622",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "required = [\"final_weights_no_curv\", \"final_weights_with_curv\"]\n",
    "for name in required:\n",
    "    if name not in locals():\n",
    "        raise ValueError(f\"{name} not found. run the comparison training cell first.\")\n",
    "\n",
    "w_no  = final_weights_no_curv\n",
    "w_cur = final_weights_with_curv\n",
    "\n",
    "if w_no.shape[0] != V.shape[1] or w_cur.shape[0] != V.shape[1]:\n",
    "    raise ValueError(\"mismatch between number of weights and number of wallets.\")\n",
    "\n",
    "print(\" check the weights \")\n",
    "print(\"[no_curv] sum w:\", w_no.sum(), \"sum |w|:\", np.abs(w_no).sum(), \"min:\", w_no.min(), \"max:\", w_no.max())\n",
    "print(\"[with_curv] sum w:\", w_cur.sum(), \"sum |w|:\", np.abs(w_cur).sum(), \"min:\", w_cur.min(), \"max:\", w_cur.max())\n",
    "\n",
    "num_chunks = NUM_CHUNKS\n",
    "\n",
    "# validation window\n",
    "s_val, e_val = get_chunk_indices(num_chunks - 1)\n",
    "val_slice = slice(s_val + 1, e_val)\n",
    "\n",
    "# test window\n",
    "s_test, e_test = get_chunk_indices(num_chunks)\n",
    "test_slice = slice(s_test + 1, e_test)\n",
    "\n",
    "times_val = grid_times[val_slice]\n",
    "times_test = grid_times[test_slice]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(w_no,  bins=50, alpha=0.5, label=\"no_curv\")\n",
    "plt.hist(w_cur, bins=50, alpha=0.5, label=\"with_curv\")\n",
    "plt.xlabel(\"weight\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"distribution of learned long-short weights\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "top_k = 20\n",
    "sorted_idx_cur = np.argsort(-np.abs(w_cur))\n",
    "top_idx = sorted_idx_cur[:top_k]\n",
    "top_weights_cur = w_cur[top_idx]\n",
    "top_wallets = [wallet_list[i] for i in top_idx]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(range(top_k), top_weights_cur)\n",
    "plt.xticks(range(top_k), [w[:8] + \"...\" for w in top_wallets], rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"weight\")\n",
    "plt.title(f\"top {top_k} wallets by long-short weight (with curvature)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "top_k_60 = 60\n",
    "top_idx_60 = sorted_idx_cur[:top_k_60]\n",
    "top_weights_cur_60 = w_cur[top_idx_60]\n",
    "top_wallets_60 = [wallet_list[i] for i in top_idx_60]\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.bar(range(top_k_60), top_weights_cur_60)\n",
    "plt.xticks(\n",
    "    range(top_k_60),\n",
    "    [w[:8] + \"...\" for w in top_wallets_60],\n",
    "    rotation=90,\n",
    "    ha=\"right\"\n",
    ")\n",
    "plt.ylabel(\"weight\")\n",
    "plt.title(f\"top {top_k_60} wallets by long-short weight (with curvature)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "num_times = R_wallet.shape[0]\n",
    "port_step_no  = R_wallet @ w_no\n",
    "port_step_cur = R_wallet @ w_cur\n",
    "\n",
    "def build_path(port_step):\n",
    "    path = np.zeros(num_times, dtype=np.float64)\n",
    "    path[0] = 1.0\n",
    "    for t in range(1, num_times):\n",
    "        path[t] = path[t - 1] * (1.0 + port_step[t])\n",
    "    return path\n",
    "\n",
    "portfolio_full_no = build_path(port_step_no)\n",
    "portfolio_full_cur = build_path(port_step_cur)\n",
    "\n",
    "portfolio_val_no = portfolio_full_no[val_slice]\n",
    "portfolio_val_cur = portfolio_full_cur[val_slice]\n",
    "\n",
    "# helper to convert time to seconds\n",
    "def times_to_unix_seconds(times_array):\n",
    "    if np.issubdtype(times_array.dtype, np.datetime64):\n",
    "        return times_array.astype(\"datetime64[s]\").astype(\"int64\").astype(float)\n",
    "    elif np.issubdtype(times_array.dtype, np.number):\n",
    "        return times_array.astype(float)\n",
    "    else:\n",
    "        import pandas as pd\n",
    "        return pd.to_datetime(times_array).astype(np.int64) // 10**9\n",
    "\n",
    "times_val_sec = times_to_unix_seconds(np.asarray(times_val))\n",
    "\n",
    "def stats_for_window(port_step, portfolio_window, label):\n",
    "    step_ret_val = port_step[val_slice]\n",
    "    mean_r = step_ret_val.mean()\n",
    "    std_r = step_ret_val.std()\n",
    "    sharpe_val = mean_r / std_r if std_r > 0 else 0.0\n",
    "\n",
    "    if len(times_val_sec) > 1:\n",
    "        dt = np.median(np.diff(times_val_sec))\n",
    "        dt_years = dt / (365.25 * 24 * 3600)\n",
    "        n_steps_per_year = 1.0 / dt_years\n",
    "\n",
    "        period_years = (times_val_sec[-1] - times_val_sec[0]) / (365.25 * 24 * 3600)\n",
    "        annualized_return = (portfolio_window[-1] / portfolio_window[0])**(1 / period_years) - 1\n",
    "        ann_mean = mean_r * n_steps_per_year\n",
    "        ann_std = std_r * np.sqrt(n_steps_per_year)\n",
    "        annualized_sharpe = ann_mean / ann_std if ann_std > 0 else 0.0\n",
    "    else:\n",
    "        annualized_return = float(\"nan\")\n",
    "        annualized_sharpe = float(\"nan\")\n",
    "\n",
    "    print(f\"\\nvalidation stats [{label}]:\")\n",
    "    print(\"\\tmean step return:\", float(mean_r))\n",
    "    print(\"\\tstd step return:\", float(std_r))\n",
    "    print(\"\\tsharpe:\", float(sharpe_val))\n",
    "    print(\"\\tannualized return:\", float(annualized_return))\n",
    "    print(\"\\tannualized sharpe:\", float(annualized_sharpe))\n",
    "\n",
    "stats_for_window(port_step_no, portfolio_val_no,  \"no_curv\")\n",
    "stats_for_window(port_step_cur, portfolio_val_cur, \"with_curv\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(grid_times, portfolio_full_no, label=\"no_curv\")\n",
    "plt.plot(grid_times, portfolio_full_cur, label=\"with_curv\")\n",
    "plt.axvline(grid_times[s_val], color=\"gray\", linestyle=\"--\", label=\"validation start\")\n",
    "plt.axvline(grid_times[s_test], color=\"red\", linestyle=\"--\", label=\"test start\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"unit value\")\n",
    "plt.title(\"portfolio over entire horizon: no_curv vs with_curv\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(times_val, portfolio_val_no / portfolio_val_no[0],  label=\"no_curv\")\n",
    "plt.plot(times_val, portfolio_val_cur / portfolio_val_cur[0], label=\"with_curv\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"unit value\")\n",
    "plt.title(\"validation window: no_curv vs with_curv\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41432739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_wallets(weights, label, top_n=40):\n",
    "    # fucntion that makes a nice table for printing\n",
    "    print(f\"\\n TOP {top_n} WALLETS with {label} \")\n",
    "    abs_sorted_idx = np.argsort(-np.abs(weights))[:top_n]\n",
    "\n",
    "    rows = []\n",
    "    for rank, idx in enumerate(abs_sorted_idx, start=1):\n",
    "        # wallet address string\n",
    "        addr = wallet_list[idx]\n",
    "        # signed weight\n",
    "        w    = float(weights[idx])\n",
    "        rows.append((rank, idx, addr, w, abs(w)))\n",
    "\n",
    "    # pretty print the results that we get\n",
    "    print(f\"{'rank':>4} | {'idx':>5} | {'wallet':<48} | {'weight':>12} | {'abs(weight)':>12}\")\n",
    "    for r, i, addr, w, aw in rows:\n",
    "        print(f\"{r:4d} | {i:5d} | {addr:<48} | {w:10f} | {aw:10f}\")\n",
    "\n",
    "# print for both models\n",
    "show_top_wallets(final_weights_no_curv,  \"NO CURVATURE\")\n",
    "show_top_wallets(final_weights_with_curv, \"WITH CURVATURE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2e633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict to wallets of interest\n",
    "df_all = df_trades[df_trades[\"proxy_wallet\"].isin(wallet_list)].copy()\n",
    "\n",
    "# ensure required columns exist\n",
    "required_cols = [\"proxy_wallet\", \"condition_id\", \"event_id\", \"series_id\"]\n",
    "missing = [c for c in required_cols if c not in df_all.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"missing columns in df_trades: {missing}\")\n",
    "\n",
    "# get the trade counts per wallet condition combination\n",
    "df_cond_counts = (\n",
    "    df_all\n",
    "    .groupby([\"proxy_wallet\", \"condition_id\"], dropna=False)\n",
    "    .size()\n",
    "    .rename(\"trade_count\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# make the mapping\n",
    "cond_map = (\n",
    "    df_all[[\"condition_id\", \"event_id\", \"series_id\"]]\n",
    "    .drop_duplicates(subset=[\"condition_id\"])\n",
    ")\n",
    "\n",
    "df_cond_counts = df_cond_counts.merge(cond_map, on=\"condition_id\", how=\"left\")\n",
    "\n",
    "# aggregate\n",
    "df_event_counts = (\n",
    "    df_cond_counts\n",
    "    .groupby([\"proxy_wallet\", \"event_id\"], dropna=False)[\"trade_count\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# aggregate\n",
    "df_series_counts = (\n",
    "    df_cond_counts\n",
    "    .groupby([\"proxy_wallet\", \"series_id\"], dropna=False)[\"trade_count\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "wallet_series_counts_wide = df_series_counts.pivot_table(\n",
    "    index=\"proxy_wallet\",\n",
    "    columns=\"series_id\",\n",
    "    values=\"trade_count\",\n",
    "    fill_value=0,\n",
    "    aggfunc=\"sum\"\n",
    ")\n",
    "\n",
    "wallet_event_counts_wide = df_event_counts.pivot_table(\n",
    "    index=\"proxy_wallet\",\n",
    "    columns=\"event_id\",\n",
    "    values=\"trade_count\",\n",
    "    fill_value=0,\n",
    "    aggfunc=\"sum\"\n",
    ")\n",
    "\n",
    "print(\"df_cond_counts shape:\", df_cond_counts.shape)\n",
    "print(\"df_event_counts shape:\", df_event_counts.shape)\n",
    "print(\"df_series_counts shape:\", df_series_counts.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07300a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46f2d09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
