{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mysql.connector\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import timedelta\n",
        "from collections import defaultdict\n",
        "from bisect import bisect_left, bisect_right\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import SAGEConv\n",
        "import networkx as nx\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# creds\n",
        "DB_HOST_NAS = \"192.168.0.165\"\n",
        "DB_PORT_NAS = 3306\n",
        "DB_USER_NAS = \"teo\"\n",
        "DB_PASSWORD_NAS = \"password\"\n",
        "DB_NAME_NAS = \"polymarket_project\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the trades and mappings\n",
        "\n",
        "TRADES_CACHE_PATH = \"cached_trades.parquet\"\n",
        "MAPPINGS_CACHE_PATH = \"cached_mappings.pkl\"\n",
        "\n",
        "# true to save \n",
        "SAVE_CACHE = False  \n",
        "\n",
        "# true to load from cache\n",
        "LOAD_FROM_CACHE = True  \n",
        "\n",
        "if SAVE_CACHE:\n",
        "    print(\"Saving trades to local cache...\")\n",
        "    df_trades.to_parquet(TRADES_CACHE_PATH, index=False)\n",
        "    \n",
        "    # mapping tables and series titles\n",
        "    import pickle\n",
        "    mappings = {\n",
        "        \"df_markets\": df_markets,\n",
        "        \"df_market_events\": df_market_events,\n",
        "        \"df_event_series\": df_event_series,\n",
        "        \"series_id_to_title\": series_id_to_title\n",
        "    }\n",
        "    with open(MAPPINGS_CACHE_PATH, \"wb\") as f:\n",
        "        pickle.dump(mappings, f)\n",
        "    \n",
        "    print(f\"Saved {len(df_trades)} trades to {TRADES_CACHE_PATH}\")\n",
        "    print(f\"Saved mappings to {MAPPINGS_CACHE_PATH}\")\n",
        "\n",
        "elif LOAD_FROM_CACHE:\n",
        "    if os.path.exists(TRADES_CACHE_PATH) and os.path.exists(MAPPINGS_CACHE_PATH):\n",
        "        print(\"Loading trades from local cache...\")\n",
        "        df_trades = pd.read_parquet(TRADES_CACHE_PATH)\n",
        "        \n",
        "        import pickle\n",
        "        with open(MAPPINGS_CACHE_PATH, \"rb\") as f:\n",
        "            mappings = pickle.load(f)\n",
        "        \n",
        "        df_markets = mappings[\"df_markets\"]\n",
        "        df_market_events = mappings[\"df_market_events\"]\n",
        "        df_event_series = mappings[\"df_event_series\"]\n",
        "        series_id_to_title = mappings[\"series_id_to_title\"]\n",
        "        \n",
        "        # if trade_time not in df_trades recreate it\n",
        "        if \"trade_time\" not in df_trades.columns:\n",
        "            if np.issubdtype(df_trades[\"timestamp\"].dtype, np.number):\n",
        "                df_trades[\"trade_time\"] = pd.to_datetime(df_trades[\"timestamp\"], unit=\"s\")\n",
        "            else:\n",
        "                df_trades[\"trade_time\"] = pd.to_datetime(df_trades[\"timestamp\"])\n",
        "        \n",
        "        print(f\"Loaded {len(df_trades)} trades from cache\")\n",
        "        print(f\"Series loaded: {len(series_id_to_title)}\")\n",
        "    else:\n",
        "        print(\"Cache files not found. Run with SAVE_CACHE=True first after loading from DB.\")\n",
        "else:\n",
        "    print(\"Using data from database (already loaded in Cell 2)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  market/event/series data enrichment\n",
        "\n",
        "df_trades = df_trades.sort_values(\"trade_time\").reset_index(drop=True)\n",
        "df_trades[\"timestamp\"] = pd.to_numeric(df_trades[\"timestamp\"], errors=\"coerce\")\n",
        "df_trades = df_trades.dropna(subset=[\"timestamp\"])\n",
        "\n",
        "# condition and market merge\n",
        "df_markets_trim = df_markets[[\"id\", \"conditionId\"]].dropna(subset=[\"conditionId\"]).copy()\n",
        "df_markets_trim.rename(columns={\"id\": \"market_id\"}, inplace=True)\n",
        "df_trades = df_trades.merge(df_markets_trim, how=\"left\", left_on=\"condition_id\", right_on=\"conditionId\")\n",
        "df_trades.drop(columns=[\"conditionId\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "# market and event merge\n",
        "if not df_market_events.empty:\n",
        "    df_trades = df_trades.merge(\n",
        "        df_market_events[[\"market_id\", \"event_id\"]].dropna(),\n",
        "        how=\"left\", on=\"market_id\"\n",
        "    )\n",
        "else:\n",
        "    df_trades[\"event_id\"] = np.nan\n",
        "\n",
        "# event and series merge\n",
        "if not df_event_series.empty:\n",
        "    df_trades = df_trades.merge(\n",
        "        df_event_series[[\"event_id\", \"series_id\"]].dropna(),\n",
        "        how=\"left\", on=\"event_id\"\n",
        "    )\n",
        "else:\n",
        "    df_trades[\"series_id\"] = np.nan\n",
        "\n",
        "# convert series id to sting\n",
        "df_trades[\"series_id\"] = df_trades[\"series_id\"].astype(str)\n",
        "\n",
        "print(\"Trades with series info:\")\n",
        "print(df_trades[[\"proxy_wallet\", \"condition_id\", \"series_id\"]].head())\n",
        "print(\"\\nUnique series:\", df_trades[\"series_id\"].nunique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Params\n",
        "MIN_TRADES_PER_WALLET = 10\n",
        "MIN_RELATIONSHIP_WEIGHT = 0.50    \n",
        "\n",
        "MAX_GAP_SECONDS = 15\n",
        "CO_TRADE_EXTENSION_SECONDS = 40   \n",
        "\n",
        "# Training parameters\n",
        "NUM_CHUNKS = 6\n",
        "NUM_EPOCHS = 500\n",
        "EARLY_STOP_PATIENCE = 100\n",
        "HIDDEN_DIM = 16\n",
        "NUM_LAYERS = 4\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "# Minimum requirements for a series to be analyzed\n",
        "MIN_WALLETS_IN_SERIES = 20\n",
        "MIN_EDGES_IN_GRAPH = 10\n",
        "\n",
        "TOP_N_SERIES = 50\n",
        "\n",
        "feature_flags = {\n",
        "    # Base feats\n",
        "    \"use_return_mean_std\": True,\n",
        "    \"use_log_trades_per_hour\": True,\n",
        "    \"use_avg_trade_size\": True,\n",
        "    \"use_avg_trade_price\": True,\n",
        "    \"use_max_drawdown_pct\": True,\n",
        "    \n",
        "    # Graph feats\n",
        "    \"use_degree_in\": True,\n",
        "    \"use_degree_out\": True,\n",
        "    \"use_leader_ratio\": True, # deg_out / (1 + deg_in) \n",
        "    \n",
        "    # Curvature feats\n",
        "    \"use_orc_curvature\": False, # Heavy ORC \n",
        "    \"use_local_curvature\": True # Fast local clustering curvature\n",
        "}\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"\\tMIN_TRADES_PER_WALLET: {MIN_TRADES_PER_WALLET}\")\n",
        "print(f\"\\tMIN_RELATIONSHIP_WEIGHT: {MIN_RELATIONSHIP_WEIGHT}\")\n",
        "print(f\"\\tMAX_GAP_SECONDS: {MAX_GAP_SECONDS}\")\n",
        "print(f\"\\tCO_TRADE_EXTENSION_SECONDS: {CO_TRADE_EXTENSION_SECONDS}\")\n",
        "print(f\"\\tTOP_N_SERIES: {TOP_N_SERIES if TOP_N_SERIES else 'ALL'}\")\n",
        "print(f\"\\nFeature flags:\")\n",
        "for flag, val in feature_flags.items():\n",
        "    print(f\"{flag}: {val}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# find the series that have enough trades\n",
        "# to include in our dataset\n",
        "\n",
        "# count trades \n",
        "series_trade_counts = df_trades.groupby(\"series_id\").size().sort_values(ascending=False)\n",
        "\n",
        "# count wallets per series\n",
        "series_wallet_counts = df_trades.groupby(\"series_id\")[\"proxy_wallet\"].nunique().sort_values(ascending=False)\n",
        "\n",
        "# filter to series with enough wallets\n",
        "valid_series = series_wallet_counts[series_wallet_counts >= MIN_WALLETS_IN_SERIES].index.tolist()\n",
        "\n",
        "# remove nan and sort by volume\n",
        "valid_series = [s for s in valid_series if s != 'nan' and pd.notna(s)]\n",
        "valid_series = sorted(valid_series, key=lambda s: series_trade_counts.get(s, 0), reverse=True)\n",
        "\n",
        "# apply the filter\n",
        "if TOP_N_SERIES is not None and TOP_N_SERIES > 0:\n",
        "    valid_series = valid_series[:TOP_N_SERIES]\n",
        "    print(f\"*** LIMITING TO TOP {TOP_N_SERIES} SERIES BY TRADE VOLUME ***\\n\")\n",
        "\n",
        "print(f\"Total unique series: {df_trades['series_id'].nunique()}\")\n",
        "print(f\"Series with >= {MIN_WALLETS_IN_SERIES} wallets: {len(valid_series)}\")\n",
        "print(f\"\\nSeries to analyze ({len(valid_series)} total sort by volume:\")\n",
        "for i, sid in enumerate(valid_series[:20]):\n",
        "    title = series_id_to_title.get(sid, sid)\n",
        "    n_wallets = series_wallet_counts[sid]\n",
        "    n_trades = series_trade_counts[sid]\n",
        "    print(f\"  {i+1:>2}. {sid}: {title[:45]:<45} | {n_wallets:>5} wallets | {n_trades:>7} trades\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helpers\n",
        "\n",
        "def normalize_long_short(scores, eps=1e-8):\n",
        "    w = torch.tanh(scores)\n",
        "    w = w - w.mean()\n",
        "    sum_abs = w.abs().sum()\n",
        "    sum_abs = torch.where(\n",
        "        sum_abs < eps,\n",
        "        torch.tensor(1.0, device=w.device, dtype=w.dtype),\n",
        "        sum_abs\n",
        "    )\n",
        "    w = 2.0 * w / sum_abs\n",
        "    return w\n",
        "\n",
        "\n",
        "class GNNPortfolioModel(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim=16, num_layers=4):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList()\n",
        "        \n",
        "        if num_layers == 1:\n",
        "            self.convs.append(SAGEConv(in_dim, hidden_dim))\n",
        "        \n",
        "        else:\n",
        "            self.convs.append(SAGEConv(in_dim, hidden_dim))\n",
        "            for _ in range(num_layers - 2):\n",
        "                self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
        "            self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
        "        \n",
        "        self.score_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        h = x\n",
        "        for conv in self.convs:\n",
        "            h = conv(h, edge_index)\n",
        "            h = F.relu(h)\n",
        "        scores = self.score_mlp(h).squeeze(-1)\n",
        "        wout = normalize_long_short(scores)\n",
        "        return wout, scores\n",
        "\n",
        "\n",
        "def sharpe_loss(weights, R_steps, eps=1e-8):\n",
        "    port_ret = torch.matmul(R_steps, weights)\n",
        "    mean_ret = port_ret.mean()\n",
        "    std_ret = port_ret.std(unbiased=False) + eps\n",
        "    sharpe = mean_ret / std_ret\n",
        "    return -sharpe, sharpe, mean_ret, std_ret\n",
        "\n",
        "\n",
        "def build_co_trading_graph(df_series, wallet_list, wallet_to_idx):\n",
        "    \"\"\"\n",
        "    Build co-trading directed graph for a specific series.\n",
        "    Returns: directed_graph dict, deg_in array, deg_out array, edge_index tensor\n",
        "    \"\"\"\n",
        "    num_wallets = len(wallet_list)\n",
        "    \n",
        "    # time sort trades\n",
        "    df_series = df_series.sort_values(\"trade_time\").reset_index(drop=True)\n",
        "    \n",
        "    # trade tuples\n",
        "    cols_needed = [\"condition_id\", \"trade_time\", \"proxy_wallet\"]\n",
        "    all_trades = list(df_series[cols_needed].itertuples(index=False, name=None))\n",
        "    \n",
        "    if len(all_trades) < 10:\n",
        "        return None, None, None, None, 0\n",
        "    \n",
        "    # grp by (condition_id, wallet)\n",
        "    trades_by_cp = defaultdict(list)\n",
        "    for row in all_trades:\n",
        "        condition_id, trade_time, proxy_wallet = row\n",
        "        trades_by_cp[(condition_id, proxy_wallet)].append(trade_time)\n",
        "    \n",
        "    # continuum window construction\n",
        "    continuum_windows = []\n",
        "    for (condition_id, proxy_wallet), times in trades_by_cp.items():\n",
        "        times = sorted(times)\n",
        "        current_start = times[0]\n",
        "        current_end = times[0]\n",
        "        \n",
        "        for ts in times[1:]:\n",
        "            if (ts - current_end).total_seconds() <= MAX_GAP_SECONDS:\n",
        "                current_end = ts\n",
        "            else:\n",
        "                continuum_windows.append({\n",
        "                    \"condition_id\": condition_id,\n",
        "                    \"wallet\": proxy_wallet,\n",
        "                    \"min_ts\": current_start,\n",
        "                    \"max_ts\": current_end\n",
        "                })\n",
        "                current_start = ts\n",
        "                current_end = ts\n",
        "        \n",
        "        continuum_windows.append({\n",
        "            \"condition_id\": condition_id,\n",
        "            \"wallet\": proxy_wallet,\n",
        "            \"min_ts\": current_start,\n",
        "            \"max_ts\": current_end\n",
        "        })\n",
        "    \n",
        "    # group by condition\n",
        "    trades_by_condition = defaultdict(list)\n",
        "    for row in all_trades:\n",
        "        condition_id, trade_time, proxy_wallet = row\n",
        "        trades_by_condition[condition_id].append((trade_time, proxy_wallet))\n",
        "    \n",
        "    # sort by time\n",
        "    cond_times = {}\n",
        "    for cond_id, rows_c in trades_by_condition.items():\n",
        "        rows_c.sort(key=lambda x: x[0])\n",
        "        cond_times[cond_id] = [r[0] for r in rows_c]\n",
        "    \n",
        "    # find co-trade windows\n",
        "    total_windows_by_wallet = defaultdict(int)\n",
        "    co_trade_windows = defaultdict(lambda: defaultdict(int))\n",
        "    \n",
        "    for w in continuum_windows:\n",
        "        cond_id = w[\"condition_id\"]\n",
        "        wallet_a = w[\"wallet\"]\n",
        "        min_ts = w[\"min_ts\"]\n",
        "        max_ts = w[\"max_ts\"]\n",
        "        \n",
        "        start_ts = min_ts\n",
        "        end_ts = max_ts + timedelta(seconds=CO_TRADE_EXTENSION_SECONDS)\n",
        "        \n",
        "        total_windows_by_wallet[wallet_a] += 1\n",
        "        \n",
        "        rows_c = trades_by_condition[cond_id]\n",
        "        times_c = cond_times[cond_id]\n",
        "        \n",
        "        left = bisect_left(times_c, start_ts)\n",
        "        right = bisect_right(times_c, end_ts)\n",
        "        \n",
        "        co_wallets = set()\n",
        "        for trade_time, proxy_wallet in rows_c[left:right]:\n",
        "            if proxy_wallet != wallet_a:\n",
        "                co_wallets.add(proxy_wallet)\n",
        "        \n",
        "        for wallet_b in co_wallets:\n",
        "            co_trade_windows[wallet_a][wallet_b] += 1\n",
        "    \n",
        "    # directed graph with edge weight threshold\n",
        "    directed_graph = defaultdict(dict)\n",
        "    \n",
        "    for wallet_a, targets in co_trade_windows.items():\n",
        "        if wallet_a not in wallet_to_idx:\n",
        "            continue\n",
        "        total_windows = total_windows_by_wallet[wallet_a]\n",
        "        if total_windows == 0:\n",
        "            continue\n",
        "        for wallet_b, count in targets.items():\n",
        "            if wallet_b not in wallet_to_idx:\n",
        "                continue\n",
        "            pct_overlap = count / total_windows\n",
        "            if pct_overlap >= MIN_RELATIONSHIP_WEIGHT:\n",
        "                directed_graph[wallet_a][wallet_b] = float(pct_overlap)\n",
        "    \n",
        "    # compute deg \n",
        "    deg_out = np.zeros(num_wallets, dtype=np.float32)\n",
        "    deg_in = np.zeros(num_wallets, dtype=np.float32)\n",
        "    \n",
        "    for w_a, neighbors in directed_graph.items():\n",
        "        if w_a not in wallet_to_idx:\n",
        "            continue\n",
        "        i = wallet_to_idx[w_a]\n",
        "        deg_out[i] = len(neighbors)\n",
        "        for w_b in neighbors.keys():\n",
        "            if w_b in wallet_to_idx:\n",
        "                j = wallet_to_idx[w_b]\n",
        "                deg_in[j] += 1.0\n",
        "    \n",
        "    # make edge index\n",
        "    edge_src = []\n",
        "    edge_dst = []\n",
        "    \n",
        "    for w_a, neighbors in directed_graph.items():\n",
        "        i = wallet_to_idx[w_a]\n",
        "        for w_b in neighbors.keys():\n",
        "            j = wallet_to_idx[w_b]\n",
        "            edge_src.append(i)\n",
        "            edge_dst.append(j)\n",
        "    \n",
        "    num_edges = len(edge_src)\n",
        "    \n",
        "    if num_edges == 0:\n",
        "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
        "    else:\n",
        "        edge_index = torch.tensor([edge_src, edge_dst], dtype=torch.long)\n",
        "    \n",
        "    return directed_graph, deg_in, deg_out, edge_index, num_edges\n",
        "\n",
        "\n",
        "def compute_local_curvature(directed_graph, wallet_list, wallet_to_idx):\n",
        "    \"\"\"\n",
        "    Compute local curvature proxy for each wallet.\n",
        "    \"\"\"\n",
        "    num_wallets = len(wallet_list)\n",
        "    local_curv_vec = np.zeros(num_wallets, dtype=np.float32)\n",
        "    \n",
        "    # undirected graph for curvature\n",
        "    G = nx.Graph()\n",
        "    for w_a, neighbors in directed_graph.items():\n",
        "        for w_b, weight in neighbors.items():\n",
        "            if w_a == w_b:\n",
        "                continue\n",
        "            if G.has_edge(w_a, w_b):\n",
        "                G[w_a][w_b][\"weight\"] = max(G[w_a][w_b][\"weight\"], weight)\n",
        "            else:\n",
        "                G.add_edge(w_a, w_b, weight=weight)\n",
        "    \n",
        "    if G.number_of_edges() == 0:\n",
        "        return local_curv_vec, 0.0, 0.0, 0.0\n",
        "    \n",
        "    # cluster coeffs\n",
        "    clustering = nx.clustering(G, weight=\"weight\")\n",
        "    \n",
        "    for w, idx in wallet_to_idx.items():\n",
        "        C_i = float(clustering.get(w, 0.0))\n",
        "        local_curv_vec[idx] = 1.0 - C_i\n",
        "    \n",
        "    # filter active wallets\n",
        "    active_curvs = [local_curv_vec[wallet_to_idx[w]] for w in G.nodes() if w in wallet_to_idx]\n",
        "    if active_curvs:\n",
        "        curv_min = float(np.min(active_curvs))\n",
        "        curv_max = float(np.max(active_curvs))\n",
        "        curv_mean = float(np.mean(active_curvs))\n",
        "    else:\n",
        "        curv_min, curv_max, curv_mean = 0.0, 0.0, 0.0\n",
        "    \n",
        "    return local_curv_vec, curv_min, curv_max, curv_mean\n",
        "\n",
        "\n",
        "def compute_orc_curvature(directed_graph, wallet_list, wallet_to_idx):\n",
        "    num_wallets = len(wallet_list)\n",
        "    orc_vec = np.zeros(num_wallets, dtype=np.float32)\n",
        "    \n",
        "    try:\n",
        "        # windows uses spawn not fork for orc\n",
        "        import multiprocessing as mp\n",
        "        _original_get_context = mp.get_context\n",
        "        def _patched_get_context(method=None):\n",
        "            if method == 'fork':\n",
        "                method = 'spawn'  \n",
        "            return _original_get_context(method)\n",
        "        mp.get_context = _patched_get_context\n",
        "        \n",
        "        from GraphRicciCurvature.OllivierRicci import OllivierRicci  \n",
        "    except ImportError:\n",
        "        print(\"GraphRicciCurvature not installed.\")\n",
        "        return orc_vec, 0.0, 0.0, 0.0\n",
        "    \n",
        "    # undirected\n",
        "    G = nx.Graph()\n",
        "    for w_a, neighbors in directed_graph.items():\n",
        "        for w_b, weight in neighbors.items():\n",
        "            if w_a == w_b:\n",
        "                continue\n",
        "            w_val = float(weight)\n",
        "            if G.has_edge(w_a, w_b):\n",
        "                G[w_a][w_b][\"weight\"] = 0.5 * (G[w_a][w_b][\"weight\"] + w_val)\n",
        "            else:\n",
        "                G.add_edge(w_a, w_b, weight=w_val)\n",
        "    \n",
        "    if G.number_of_edges() == 0:\n",
        "        return orc_vec, 0.0, 0.0, 0.0\n",
        "    \n",
        "    try:\n",
        "        \n",
        "        # try to compute the curvature\n",
        "        orc = OllivierRicci(G, alpha=0.5, verbose=\"ERROR\", proc=1)\n",
        "        orc.compute_ricci_curvature()\n",
        "        \n",
        "\n",
        "        # get the curvature for each node\n",
        "        node_orc = {n: data.get(\"ricciCurvature\", 0.0) for n, data in G.nodes(data=True)}\n",
        "        for w, idx in wallet_to_idx.items():\n",
        "            orc_vec[idx] = float(node_orc.get(w, 0.0))\n",
        "        \n",
        "        active_orcs = [orc_vec[wallet_to_idx[w]] for w in G.nodes() if w in wallet_to_idx]\n",
        "        if active_orcs:\n",
        "            orc_min = float(np.min(active_orcs))\n",
        "            orc_max = float(np.max(active_orcs))\n",
        "            orc_mean = float(np.mean(active_orcs))\n",
        "        else:\n",
        "            orc_min, orc_max, orc_mean = 0.0, 0.0, 0.0\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"ORC computation failed: {e}\")\n",
        "        orc_min, orc_max, orc_mean = 0.0, 0.0, 0.0\n",
        "    \n",
        "    return orc_vec, orc_min, orc_max, orc_mean\n",
        "\n",
        "\n",
        "print(\"Helper functions defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analysis function\n",
        "\n",
        "def analyze_series(series_id, df_trades, verbose=False):\n",
        "    series_title = series_id_to_title.get(series_id, series_id)\n",
        "    \n",
        "    # filter to this series\n",
        "    df_series = df_trades[df_trades[\"series_id\"] == series_id].copy()\n",
        "    \n",
        "    if len(df_series) < 100:\n",
        "        if verbose:\n",
        "            print(f\"Skipping {series_title} only {len(df_series)} trades\")\n",
        "        return None\n",
        "    \n",
        "    df_series = df_series.sort_values(\"trade_time\").reset_index(drop=True)\n",
        "    \n",
        "    # get wallets with enough trades\n",
        "    wallet_counts = df_series.groupby(\"proxy_wallet\").size()\n",
        "    eligible_wallets = set(wallet_counts[wallet_counts >= MIN_TRADES_PER_WALLET].index)\n",
        "    \n",
        "    if len(eligible_wallets) < MIN_WALLETS_IN_SERIES:\n",
        "        if verbose:\n",
        "            print(f\"Skipping {series_title}: only {len(eligible_wallets)} eligible wallets\")\n",
        "        return None\n",
        "    \n",
        "    df_series = df_series[df_series[\"proxy_wallet\"].isin(eligible_wallets)].copy()\n",
        "    \n",
        "    # make mappings\n",
        "    wallet_list = sorted(eligible_wallets)\n",
        "    wallet_to_idx = {w: i for i, w in enumerate(wallet_list)}\n",
        "    num_wallets = len(wallet_list)\n",
        "    \n",
        "    # make time grid\n",
        "    unique_times = pd.Index(df_series[\"trade_time\"].sort_values().unique())\n",
        "    num_grid = min(200, len(unique_times))\n",
        "    \n",
        "    if num_grid < 2 * NUM_CHUNKS:\n",
        "        if verbose:\n",
        "            print(f\"Skipping {series_title}: not enough time points ({num_grid})\")\n",
        "        return None\n",
        "    \n",
        "    grid_indices = np.linspace(0, len(unique_times) - 1, num_grid).astype(int)\n",
        "    grid_times = unique_times[grid_indices]\n",
        "    \n",
        "    # token price paths\n",
        "    df_prices = df_series[[\"trade_time\", \"asset\", \"price\"]].copy()\n",
        "    df_prices[\"price\"] = df_prices[\"price\"].astype(float)\n",
        "    \n",
        "    token_price_paths = {}\n",
        "    for token_id, g in df_prices.groupby(\"asset\", sort=False):\n",
        "        g = g.sort_values(\"trade_time\")\n",
        "        g_token = g[[\"trade_time\", \"price\"]].groupby(\"trade_time\")[\"price\"].last().to_frame()\n",
        "        token_price_paths[token_id] = g_token\n",
        "    \n",
        "    token_list = sorted(token_price_paths.keys())\n",
        "    token_to_idx = {tok: i for i, tok in enumerate(token_list)}\n",
        "    num_tokens = len(token_list)\n",
        "    \n",
        "    # price grid\n",
        "    def prices_on_grid(df_token, grid_times):\n",
        "        tok_times = df_token.index.to_numpy()\n",
        "        tok_prices = df_token[\"price\"].to_numpy()\n",
        "        if tok_times.size == 0:\n",
        "            return np.zeros(len(grid_times), dtype=np.float32)\n",
        "        idx = np.searchsorted(tok_times, grid_times, side=\"right\") - 1\n",
        "        idx[idx < 0] = 0\n",
        "        idx = np.clip(idx, 0, tok_times.size - 1)\n",
        "        return tok_prices[idx].astype(np.float32)\n",
        "    \n",
        "    price_grid = np.zeros((num_tokens, num_grid), dtype=np.float32)\n",
        "    for tok, i_tok in token_to_idx.items():\n",
        "        price_grid[i_tok, :] = prices_on_grid(token_price_paths[tok], grid_times)\n",
        "    \n",
        "    # we infer initial holdings here\n",
        "    df_wallet = df_series[[\"trade_time\", \"proxy_wallet\", \"asset\", \"side\", \"size\", \"price\"]].copy()\n",
        "    df_wallet[\"size\"] = df_wallet[\"size\"].astype(float)\n",
        "    df_wallet[\"price\"] = df_wallet[\"price\"].astype(float)\n",
        "    df_wallet = df_wallet.sort_values(\"trade_time\").reset_index(drop=True)\n",
        "    \n",
        "    df_wallet[\"share_change\"] = np.where(df_wallet[\"side\"] == \"BUY\", df_wallet[\"size\"], -df_wallet[\"size\"])\n",
        "    df_wallet[\"cash_change\"] = np.where(\n",
        "        df_wallet[\"side\"] == \"BUY\",\n",
        "        -df_wallet[\"size\"] * df_wallet[\"price\"],\n",
        "        df_wallet[\"size\"] * df_wallet[\"price\"]\n",
        "    )\n",
        "    \n",
        "    wallet_initial_shares = defaultdict(dict)\n",
        "    for (wallet, asset), grp in df_wallet.groupby([\"proxy_wallet\", \"asset\"]):\n",
        "        cum_shares = np.cumsum(grp[\"share_change\"].values)\n",
        "        min_cum = cum_shares.min()\n",
        "        wallet_initial_shares[wallet][asset] = max(0.0, -float(min_cum))\n",
        "    \n",
        "    wallet_initial_cash = {}\n",
        "    for wallet, grp in df_wallet.groupby(\"proxy_wallet\"):\n",
        "        cum_cash = np.cumsum(grp[\"cash_change\"].values)\n",
        "        min_cash = cum_cash.min()\n",
        "        wallet_initial_cash[wallet] = max(0.0, -float(min_cash))\n",
        "    \n",
        "    # compute portfolio values over time\n",
        "    V = np.zeros((num_grid, num_wallets), dtype=np.float64)\n",
        "    \n",
        "    time_arr = df_wallet[\"trade_time\"].to_numpy()\n",
        "    wallet_arr = df_wallet[\"proxy_wallet\"].to_numpy()\n",
        "    asset_arr = df_wallet[\"asset\"].to_numpy()\n",
        "    side_arr = df_wallet[\"side\"].to_numpy()\n",
        "    size_arr = df_wallet[\"size\"].to_numpy(dtype=float)\n",
        "    price_arr = df_wallet[\"price\"].to_numpy(dtype=float)\n",
        "    n_trades = len(df_wallet)\n",
        "    \n",
        "    wallet_cash = {w: float(wallet_initial_cash.get(w, 0.0)) for w in wallet_list}\n",
        "    wallet_pos = {w: dict(wallet_initial_shares.get(w, {})) for w in wallet_list}\n",
        "    \n",
        "    def compute_wallet_value(wallet, k):\n",
        "        val = wallet_cash[wallet]\n",
        "        pos = wallet_pos[wallet]\n",
        "        for asset, shares in pos.items():\n",
        "            if shares == 0.0:\n",
        "                continue\n",
        "            tok_idx = token_to_idx.get(asset)\n",
        "            if tok_idx is None:\n",
        "                continue\n",
        "            val += shares * price_grid[tok_idx, k]\n",
        "        return val\n",
        "    \n",
        "    trade_idx = 0\n",
        "    active_wallets_set = set()\n",
        "    \n",
        "    for k, t in enumerate(grid_times):\n",
        "        if k > 0:\n",
        "            V[k, :] = V[k - 1, :]\n",
        "        \n",
        "        while trade_idx < n_trades and time_arr[trade_idx] <= t:\n",
        "            w = wallet_arr[trade_idx]\n",
        "            a = asset_arr[trade_idx]\n",
        "            s = side_arr[trade_idx]\n",
        "            q = size_arr[trade_idx]\n",
        "            p = price_arr[trade_idx]\n",
        "            \n",
        "            if w not in wallet_to_idx:\n",
        "                trade_idx += 1\n",
        "                continue\n",
        "            \n",
        "            if a not in wallet_pos[w]:\n",
        "                wallet_pos[w][a] = 0.0\n",
        "            \n",
        "            if s == \"BUY\":\n",
        "                wallet_pos[w][a] += q\n",
        "                wallet_cash[w] -= q * p\n",
        "            else:\n",
        "                wallet_pos[w][a] -= q\n",
        "                wallet_cash[w] += q * p\n",
        "            \n",
        "            if wallet_pos[w][a] < 0 and wallet_pos[w][a] > -1e-9:\n",
        "                wallet_pos[w][a] = 0.0\n",
        "            if wallet_cash[w] < 0 and wallet_cash[w] > -1e-9:\n",
        "                wallet_cash[w] = 0.0\n",
        "            \n",
        "            active_wallets_set.add(w)\n",
        "            trade_idx += 1\n",
        "        \n",
        "        for w in active_wallets_set:\n",
        "            j = wallet_to_idx.get(w)\n",
        "            if j is not None:\n",
        "                V[k, j] = compute_wallet_value(w, k)\n",
        "        \n",
        "        active_wallets_set.clear()\n",
        "    \n",
        "    # normalize to unit value paths and compute returns\n",
        "    V_unit = V.copy()\n",
        "    for j in range(num_wallets):\n",
        "        col = V[:, j]\n",
        "        positive_idx = np.where(col > 0)[0]\n",
        "        if len(positive_idx) > 0:\n",
        "            start_val = col[positive_idx[0]]\n",
        "            if start_val > 0:\n",
        "                V_unit[:, j] = col / start_val\n",
        "    \n",
        "    R_wallet = np.zeros((num_grid, num_wallets), dtype=np.float64)\n",
        "    V_prev = V_unit[:-1, :]\n",
        "    V_curr = V_unit[1:, :]\n",
        "    mask = V_prev > 0\n",
        "    delta = V_curr - V_prev\n",
        "    R_step = np.zeros_like(V_prev)\n",
        "    np.divide(delta, V_prev, out=R_step, where=mask)\n",
        "    R_wallet[1:, :] = R_step\n",
        "    \n",
        "    # chunk indices\n",
        "    chunk_edges = np.linspace(0, num_grid, NUM_CHUNKS + 1, dtype=int)\n",
        "    \n",
        "    def get_chunk_indices(k):\n",
        "        return int(chunk_edges[k - 1]), int(chunk_edges[k])\n",
        "    \n",
        "    # cut off for features/graph: end of chunk NUM_CHUNKS-2\n",
        "    cut_start, cut_end = get_chunk_indices(NUM_CHUNKS - 2)\n",
        "    cut_idx = cut_end - 1\n",
        "    t_cut = grid_times[cut_idx]\n",
        "    \n",
        "    # make the feats up to t_cut\n",
        "    r_up_to_cut = R_wallet[1:cut_end, :]\n",
        "    ret_mean = r_up_to_cut.mean(axis=0).astype(np.float32)\n",
        "    ret_std = r_up_to_cut.std(axis=0).astype(np.float32)\n",
        "    \n",
        "    # trade activity features\n",
        "    df_wallet_tcut = df_wallet[df_wallet[\"trade_time\"] <= t_cut]\n",
        "    \n",
        "    def reindex_to_wallets(series, default=0.0):\n",
        "        return series.reindex(wallet_list).fillna(default).astype(np.float32).to_numpy()\n",
        "    \n",
        "    if not df_wallet_tcut.empty:\n",
        "        agg_time = df_wallet_tcut.groupby(\"proxy_wallet\")[\"trade_time\"].agg([\"count\", \"min\", \"max\"])\n",
        "        horizon_hours = (agg_time[\"max\"] - agg_time[\"min\"]).dt.total_seconds() / 3600.0\n",
        "        horizon_hours = horizon_hours.replace(0, 1.0 / 3600.0)\n",
        "        trades_per_hour = agg_time[\"count\"] / horizon_hours\n",
        "        log_trades_per_hour = np.log1p(trades_per_hour)\n",
        "    else:\n",
        "        log_trades_per_hour = pd.Series([], dtype=float)\n",
        "    \n",
        "    log_trades = reindex_to_wallets(log_trades_per_hour)\n",
        "    avg_size = reindex_to_wallets(df_wallet_tcut.groupby(\"proxy_wallet\")[\"size\"].mean())\n",
        "    avg_price = reindex_to_wallets(df_wallet_tcut.groupby(\"proxy_wallet\")[\"price\"].mean())\n",
        "    \n",
        "    # max drawdown calc\n",
        "    #\n",
        "    max_drawdown = np.zeros(num_wallets, dtype=np.float32)\n",
        "    for j in range(num_wallets):\n",
        "        v = V_unit[:cut_end, j]\n",
        "        if np.any(v > 0):\n",
        "            running_max = np.maximum.accumulate(v)\n",
        "            with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "                dd = np.where(running_max > 0, v / running_max - 1.0, 0.0)\n",
        "            max_drawdown[j] = float(-dd.min()) if dd.min() < 0 else 0.0\n",
        "    \n",
        "    # make co trading graph\n",
        "    df_graph = df_series[df_series[\"trade_time\"] <= t_cut].copy()\n",
        "    directed_graph, deg_in, deg_out, edge_index, num_edges = build_co_trading_graph(\n",
        "        df_graph, wallet_list, wallet_to_idx\n",
        "    )\n",
        "    \n",
        "    if num_edges < MIN_EDGES_IN_GRAPH:\n",
        "        if verbose:\n",
        "            print(f\"Skipping {series_title} since it has only {num_edges} edges in graph\")\n",
        "        return None\n",
        "    \n",
        "    # compute curvatures\n",
        "    local_curv_vec = np.zeros(num_wallets, dtype=np.float32)\n",
        "    orc_curv_vec = np.zeros(num_wallets, dtype=np.float32)\n",
        "    curv_min, curv_max, curv_mean = 0.0, 0.0, 0.0\n",
        "    orc_min, orc_max, orc_mean = 0.0, 0.0, 0.0\n",
        "    \n",
        "    if feature_flags.get(\"use_local_curvature\", True):\n",
        "        local_curv_vec, curv_min, curv_max, curv_mean = compute_local_curvature(\n",
        "            directed_graph, wallet_list, wallet_to_idx\n",
        "        )\n",
        "    \n",
        "    if feature_flags.get(\"use_orc_curvature\", False):\n",
        "        orc_curv_vec, orc_min, orc_max, orc_mean = compute_orc_curvature(\n",
        "            directed_graph, wallet_list, wallet_to_idx\n",
        "        )\n",
        "    \n",
        "    # assemble base feats\n",
        "    feature_list_base = []\n",
        "    \n",
        "    if feature_flags.get(\"use_return_mean_std\", True):\n",
        "        feature_list_base.append(ret_mean[:, None])\n",
        "        feature_list_base.append(ret_std[:, None])\n",
        "    \n",
        "    if feature_flags.get(\"use_log_trades_per_hour\", True):\n",
        "        feature_list_base.append(log_trades[:, None])\n",
        "    \n",
        "    if feature_flags.get(\"use_avg_trade_size\", True):\n",
        "        feature_list_base.append(avg_size[:, None])\n",
        "    \n",
        "    if feature_flags.get(\"use_avg_trade_price\", True):\n",
        "        feature_list_base.append(avg_price[:, None])\n",
        "    \n",
        "    if feature_flags.get(\"use_max_drawdown_pct\", True):\n",
        "        feature_list_base.append(max_drawdown[:, None])\n",
        "    \n",
        "    if feature_flags.get(\"use_degree_out\", True):\n",
        "        feature_list_base.append(deg_out[:, None])\n",
        "    \n",
        "    if feature_flags.get(\"use_degree_in\", True):\n",
        "        feature_list_base.append(deg_in[:, None])\n",
        "\n",
        "        # lender ratio\n",
        "    leader_ratio = deg_out / (1 + deg_in)\n",
        "    \n",
        "    if feature_flags.get(\"use_leader_ratio\", True):\n",
        "        feature_list_base.append(leader_ratio[:, None])\n",
        "    \n",
        "    if len(feature_list_base) == 0:\n",
        "        feature_list_base = [ret_mean[:, None], ret_std[:, None]]\n",
        "    \n",
        "    X_base = np.concatenate(feature_list_base, axis=1).astype(np.float32)\n",
        "    \n",
        "    # assemble curve feats\n",
        "    feature_list_curv = [X_base]\n",
        "    \n",
        "    if feature_flags.get(\"use_local_curvature\", True):\n",
        "        feature_list_curv.append(local_curv_vec[:, None])\n",
        "    \n",
        "    if feature_flags.get(\"use_orc_curvature\", False):\n",
        "        feature_list_curv.append(orc_curv_vec[:, None])\n",
        "    \n",
        "    X_with_curv = np.concatenate(feature_list_curv, axis=1).astype(np.float32)\n",
        "    \n",
        "    # active wallets filtration\n",
        "    deg_total = deg_in + deg_out\n",
        "    active_mask = deg_total >= 1\n",
        "    active_idx = np.where(active_mask)[0]\n",
        "    \n",
        "    if active_idx.size < 10:\n",
        "        if verbose:\n",
        "            print(f\"Skipping {series_title} since it has only {active_idx.size} active wallets\")\n",
        "        return None\n",
        "    \n",
        "    X_no_curv_active = X_base[active_idx]\n",
        "    X_with_curv_active = X_with_curv[active_idx]\n",
        "    R_wallet_active = R_wallet[:, active_idx].astype(np.float32)\n",
        "    \n",
        "    # edge index for active wallets\n",
        "    edge_index_np = edge_index.numpy()\n",
        "    src_full, dst_full = edge_index_np[0], edge_index_np[1]\n",
        "    edge_keep_mask = active_mask[src_full] & active_mask[dst_full]\n",
        "    src_kept = src_full[edge_keep_mask]\n",
        "    dst_kept = dst_full[edge_keep_mask]\n",
        "    \n",
        "    idx_map = -np.ones(num_wallets, dtype=np.int64)\n",
        "    idx_map[active_idx] = np.arange(active_idx.size, dtype=np.int64)\n",
        "    \n",
        "    src_sub = idx_map[src_kept]\n",
        "    dst_sub = idx_map[dst_kept]\n",
        "    edge_index_active = torch.tensor(np.vstack([src_sub, dst_sub]), dtype=torch.long, device=device)\n",
        "    \n",
        "    # make train/val/test returns\n",
        "    def get_return_block(k):\n",
        "        s, e = get_chunk_indices(k)\n",
        "        if e <= s + 1:\n",
        "            return None\n",
        "        return R_wallet_active[s + 1:e, :].astype(np.float32)\n",
        "    \n",
        "    train_blocks = []\n",
        "    for i in range(1, NUM_CHUNKS - 2):\n",
        "        R_k = get_return_block(i + 1)\n",
        "        if R_k is not None:\n",
        "            train_blocks.append(R_k)\n",
        "    \n",
        "    if not train_blocks:\n",
        "        if verbose:\n",
        "            print(f\"Skipping {series_title} with no training blocks\")\n",
        "        return None\n",
        "    \n",
        "    R_train = np.concatenate(train_blocks, axis=0).astype(np.float32)\n",
        "    R_val = get_return_block(NUM_CHUNKS - 1)\n",
        "    R_test = get_return_block(NUM_CHUNKS)\n",
        "    \n",
        "    if R_val is None or R_test is None or len(R_train) < 5 or len(R_val) < 5 or len(R_test) < 5:\n",
        "        if verbose:\n",
        "            print(f\"Skipping {series_title} ad there is insufficient return data\")\n",
        "        return None\n",
        "    \n",
        "    # train models\n",
        "    def train_model(X_np, label):\n",
        "\n",
        "        # get the data in the right format\n",
        "        x_all = torch.from_numpy(X_np).float().to(device)\n",
        "        R_train_t = torch.from_numpy(R_train).float().to(device)\n",
        "        R_val_t = torch.from_numpy(R_val).float().to(device)\n",
        "        R_test_t = torch.from_numpy(R_test).float().to(device)\n",
        "        \n",
        "\n",
        "        # initialize the model\n",
        "        in_dim = x_all.size(1)\n",
        "        model = GNNPortfolioModel(in_dim=in_dim, hidden_dim=HIDDEN_DIM, num_layers=NUM_LAYERS).to(device)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "        \n",
        "        best_val_loss = float(\"inf\")\n",
        "        best_state = None\n",
        "        epochs_since_best = 0\n",
        "        \n",
        "\n",
        "        # iterate over the epochs\n",
        "        for epoch in range(1, NUM_EPOCHS + 1):\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            weights, _ = model(x_all, edge_index_active)\n",
        "            loss, _, _, _ = sharpe_loss(weights, R_train_t)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                weights_val, _ = model(x_all, edge_index_active)\n",
        "                loss_val, sharpe_val, _, _ = sharpe_loss(weights_val, R_val_t)\n",
        "            \n",
        "            if loss_val.item() < best_val_loss - 1e-6:\n",
        "                best_val_loss = loss_val.item()\n",
        "                best_state = model.state_dict()\n",
        "                epochs_since_best = 0\n",
        "            else:\n",
        "                epochs_since_best += 1\n",
        "            \n",
        "            if epochs_since_best >= EARLY_STOP_PATIENCE:\n",
        "                break\n",
        "        \n",
        "        if best_state is not None:\n",
        "            model.load_state_dict(best_state)\n",
        "        \n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            final_weights, _ = model(x_all, edge_index_active)\n",
        "            _, val_sharpe, _, _ = sharpe_loss(final_weights, R_val_t)\n",
        "            _, test_sharpe, _, _ = sharpe_loss(final_weights, R_test_t)\n",
        "        \n",
        "        # sharpe and weights return\n",
        "        return val_sharpe.item(), test_sharpe.item(), final_weights.cpu().numpy()\n",
        "    \n",
        "    # train without curvature\n",
        "    val_sharpe_no_curv, test_sharpe_no_curv, weights_no_curv = train_model(X_no_curv_active, \"no_curv\")\n",
        "    \n",
        "    # train with curvature\n",
        "    val_sharpe_with_curv, test_sharpe_with_curv, weights_with_curv = train_model(X_with_curv_active, \"with_curv\")\n",
        "    \n",
        "    # build portfolio returns for test only \n",
        "    test_start, test_end = get_chunk_indices(NUM_CHUNKS)\n",
        "    R_test_segment = R_wallet_active[test_start:test_end, :]\n",
        "    test_times = grid_times[test_start:test_end]\n",
        "    test_len = test_end - test_start\n",
        "    \n",
        "    port_ret_test_no = R_test_segment @ weights_no_curv\n",
        "    port_ret_test_curv = R_test_segment @ weights_with_curv\n",
        "    \n",
        "    cum_returns_test_no = np.zeros(test_len)\n",
        "    cum_returns_test_curv = np.zeros(test_len)\n",
        "    cum_returns_test_no[0] = 1.0\n",
        "    cum_returns_test_curv[0] = 1.0\n",
        "    for t in range(1, test_len):\n",
        "        cum_returns_test_no[t] = cum_returns_test_no[t-1] * (1 + port_ret_test_no[t])\n",
        "        cum_returns_test_curv[t] = cum_returns_test_curv[t-1] * (1 + port_ret_test_curv[t])\n",
        "    \n",
        "    return {\n",
        "        \"series_id\": series_id,\n",
        "        \"series_title\": series_title,\n",
        "        \"num_trades\": len(df_series),\n",
        "        \"num_wallets\": num_wallets,\n",
        "        \"num_active_wallets\": active_idx.size,\n",
        "        \"num_edges\": num_edges,\n",
        "        \"num_features_base\": X_base.shape[1],\n",
        "        \"num_features_curv\": X_with_curv.shape[1],\n",
        "        \"leader_ratio_mean\": float(np.mean(leader_ratio)),\n",
        "        # curve stats local\n",
        "        \"local_curv_min\": curv_min,\n",
        "        \"local_curv_max\": curv_max,\n",
        "        \"local_curv_mean\": curv_mean,\n",
        "        # orc curvature stats (if enabled)\n",
        "        \"orc_curv_min\": orc_min,\n",
        "        \"orc_curv_max\": orc_max,\n",
        "        \"orc_curv_mean\": orc_mean,\n",
        "        # performance\n",
        "        \"val_sharpe_no_curv\": val_sharpe_no_curv,\n",
        "        \"val_sharpe_with_curv\": val_sharpe_with_curv,\n",
        "        \"test_sharpe_no_curv\": test_sharpe_no_curv,\n",
        "        \"test_sharpe_with_curv\": test_sharpe_with_curv,\n",
        "        \"curv_improvement_val\": val_sharpe_with_curv - val_sharpe_no_curv,\n",
        "        \"curv_improvement_test\": test_sharpe_with_curv - test_sharpe_no_curv,\n",
        "        # returns \n",
        "        \"test_times\": test_times,\n",
        "        \"cum_returns_no_curv\": cum_returns_test_no,\n",
        "        \"cum_returns_with_curv\": cum_returns_test_curv,\n",
        "        \"final_return_no_curv\": cum_returns_test_no[-1] - 1,\n",
        "        \"final_return_with_curv\": cum_returns_test_curv[-1] - 1,\n",
        "        # per wallet data\n",
        "        \"weights_no_curv\": weights_no_curv,\n",
        "        \"weights_with_curv\": weights_with_curv,\n",
        "        \"leader_ratio_active\": leader_ratio[active_idx]\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"Per-series analysis function defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run analysis on all valid series\n",
        "\n",
        "results = []\n",
        "\n",
        "print(f\"Analyzing {len(valid_series)} series\")\n",
        "\n",
        "for i, series_id in enumerate(tqdm(valid_series, desc=\"Analyzing series\")):\n",
        "    series_title = series_id_to_title.get(series_id, series_id)\n",
        "    \n",
        "    try:\n",
        "        result = analyze_series(series_id, df_trades, verbose=False)\n",
        "        \n",
        "        if result is not None:\n",
        "            results.append(result)\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[{i+1}/{len(valid_series)}] {series_title[:40]} with the following error: - {str(e)[:50]}\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\n\\nCompleted analysis for {len(results)} series.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to Df and analyze\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "if len(df_results) == 0:\n",
        "    print(\"No results to analyze.\")\n",
        "else:\n",
        "    print(\"RESULTS\")\n",
        "    \n",
        "    print(f\"\\Series analyzed: {len(df_results)}\")\n",
        "    print(f\"\\nFlags used:\")\n",
        "    for flag, val in feature_flags.items():\n",
        "        print(f\"  {flag}: {val}\")\n",
        "    \n",
        "    print(f\"\\nOverall stats with local curvature:\")\n",
        "    print(f\"\\tstd curvature over series: {df_results['local_curv_mean'].std():.4f}\")\n",
        "    print(f\"\\tmean curvature over series: {df_results['local_curv_mean'].mean():.4f}\")\n",
        "    print(f\"\\tMin curvature: {df_results['local_curv_mean'].min():.4f}\")\n",
        "    print(f\"\\tMax curvature: {df_results['local_curv_mean'].max():.4f}\")\n",
        "    \n",
        "    if feature_flags.get(\"use_orc_curvature\", False):\n",
        "        print(f\"\\nOverall stats with ORC curvature:\")\n",
        "        print(f\"\\tMean ORC curvature over series: {df_results['orc_curv_mean'].mean():.4f}\")\n",
        "        print(f\"\\tStd ORC curvature over series: {df_results['orc_curv_mean'].std():.4f}\")\n",
        "    \n",
        "    print(f\"\\tmean val Sharpe improvement from curvature: {df_results['curv_improvement_val'].mean():.4f}\")\n",
        "    print(f\"\\tmean test Sharpe improvement from curvature: {df_results['curv_improvement_test'].mean():.4f}\")\n",
        "    \n",
        "    # How many series benefit from curvature?\n",
        "    n_val_improved = (df_results['curv_improvement_val'] > 0).sum()\n",
        "    n_test_improved = (df_results['curv_improvement_test'] > 0).sum()\n",
        "    print(f\"\\nseries where curvature improved val Sharpe: {n_val_improved} / {len(df_results)} which is {100*n_val_improved / len(df_results):.3f}%\")\n",
        "    print(f\"\\nseries where curvature improved test Sharpe: {n_test_improved} / {len(df_results)} which is {100*n_test_improved / len(df_results):.3f}%\")\n",
        "    \n",
        "    df_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# summary table and leader ratio vs curvature\n",
        "\n",
        "print(\"Top Markets by Leader Ratio\")\n",
        "summary_cols = ['series_title', 'leader_ratio_mean', 'local_curv_mean']\n",
        "summary_df = df_results[summary_cols].copy()\n",
        "summary_df.columns = ['Market', 'Leader Ratio', 'Curvature']\n",
        "summary_df = summary_df.sort_values('Leader Ratio', ascending=False).head(10)\n",
        "\n",
        "display(summary_df.reset_index(drop=True))\n",
        "\n",
        "corr_leader_curv = df_results['leader_ratio_mean'].corr(df_results['local_curv_mean'])\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ax.scatter(df_results['leader_ratio_mean'], df_results['local_curv_mean'], alpha=0.6)\n",
        "ax.set_xlabel('Leader Ratio')\n",
        "ax.set_ylabel('Curvature')\n",
        "ax.set_title(f'Leader Ratio vs Curvature with r = {corr_leader_curv:.3f}')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
